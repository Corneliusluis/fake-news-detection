{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64596a76-df44-4f7c-95ea-9cc9937875c0",
   "metadata": {},
   "source": [
    "## 1. Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7177a3c8-e45f-4bc2-aa40-5e9369463b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "STILL NEED TO ADD VERSIONS (AFTER FILE IS CONFIRMED)\n",
    "\"\"\"\n",
    "%pip install spacy\n",
    "%pip install pyarrow\n",
    "%pip install textblob\n",
    "%pip install textstat\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dba966e-68f2-4df7-a3ef-8f25fbdbe8e1",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbff736-39f8-4787-8ca3-83805762217e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OS environment\n",
    "import os\n",
    "\n",
    "# Import SparkConf class into program\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# Import SparkContext and SparkSession classes\n",
    "from pyspark import SparkContext # Spark\n",
    "from pyspark.sql import SparkSession # Spark SQL\n",
    "\n",
    "# PySpark Data Operations\n",
    "from pyspark.sql.functions import col, count, size, split, udf, pandas_udf\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Regex\n",
    "import re\n",
    "\n",
    "# Numeric operations\n",
    "import numpy as np\n",
    "\n",
    "# Define custom schema (data types) for PySpark Dataframes\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType\n",
    "\n",
    "# spaCy model for natural language processing\n",
    "import spacy\n",
    "\n",
    "# Pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Pscholinguistics\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Readability features\n",
    "import textstat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc305d0a",
   "metadata": {},
   "source": [
    "## 3. Function and Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1061797-a760-41a2-95bc-0629ff082849",
   "metadata": {},
   "source": [
    "### 3.1. clean_text function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c741c78a-9fb0-4769-bed5-2eed16d8326d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define text cleaning function\n",
    "def clean_text(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    Clean the input text string by removing unwanted elements while keeping useful punctuation.\n",
    "\n",
    "    Steps performed:\n",
    "    - Convert non-ASCII quotes/aprostrophes with ASCII equivalents\n",
    "    - Remove URLs (e.g. http://..., www...)\n",
    "    - Remove Twitter-style mentions (@username) and hashtags (#hashtag)\n",
    "    - Remove HTML entities (e.g. &nbsp;)\n",
    "    - Remove emojis and non-ASCII characters\n",
    "    - Normalize whitespace (convert multiple spaces/tabs/newlines into a single space)\n",
    "    - Trim leading and trailing spaces\n",
    "\n",
    "    Args:\n",
    "        text (str or None): The input text to clean.\n",
    "\n",
    "    Returns:\n",
    "        str: A cleaned version of the input text. If input is None, returns an empty string.\n",
    "    \"\"\"\n",
    "    \n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    \n",
    "    # Replace curly quotes/apostrophes with ASCII equivalents\n",
    "    replacements = {\n",
    "        '“': '\"', '”': '\"',\n",
    "        '‘': \"'\", '’': \"'\"\n",
    "    }\n",
    "    for curly, straight in replacements.items():\n",
    "        text = text.replace(curly, straight)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # Remove mentions and hashtags\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    \n",
    "    # Remove HTMLs\n",
    "    text = re.sub(r'&\\w+;', '', text)\n",
    "    \n",
    "    # Remove emojis and other non-ASCII symbols\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa38dd96-b7e2-4ecb-b69d-771aee837bc8",
   "metadata": {},
   "source": [
    "### 3.2. QuantityFeatures Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb37c0c7-0767-435d-b25e-57e2dc3852a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "class FeaturesSpark:\n",
    "    \"\"\"\n",
    "    Features that can be computed efficiently using PySpark.\n",
    "    \"\"\"\n",
    "    def __init__(self, text_col=\"cleaned_text\"):\n",
    "        self.text_col = text_col\n",
    "\n",
    "    def transform(self, df):\n",
    "        txt = F.coalesce(F.col(self.text_col), F.lit(\"\"))\n",
    "\n",
    "        # Character count\n",
    "        df = df.withColumn(\"num_characters\", F.length(txt))\n",
    "\n",
    "        # Capital letters\n",
    "        df = df.withColumn(\"num_capital_letters\", F.length(F.regexp_replace(txt, r\"[^A-Z]\", \"\")))\n",
    "\n",
    "        # Word count\n",
    "        df = df.withColumn(\"num_words\", F.size(F.split(txt, r\"\\s+\")))\n",
    "\n",
    "        # Sentence count\n",
    "        df = df.withColumn(\"num_sentences\", F.size(F.split(txt, r\"[.!?]+\")))\n",
    "\n",
    "        # Words per sentence\n",
    "        df = df.withColumn(\"words_per_sentence\", \n",
    "                           F.when(F.col(\"num_sentences\") > 0, F.col(\"num_words\") / F.col(\"num_sentences\"))\n",
    "                            .otherwise(F.lit(0)))\n",
    "\n",
    "        # Short sentences (<10 words)\n",
    "        df = df.withColumn(\"num_short_sentences\", \n",
    "                           F.size(F.expr(f\"filter(split({self.text_col}, '[.!?]+'), x -> size(split(x, ' ')) < 10)\")))\n",
    "\n",
    "        # Long sentences (>=20 words)\n",
    "        df = df.withColumn(\"num_long_sentences\", \n",
    "                           F.size(F.expr(f\"filter(split({self.text_col}, '[.!?]+'), x -> size(split(x, ' ')) >= 20)\")))\n",
    "\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20f1654-e1f3-449b-b2d2-422187f02d76",
   "metadata": {},
   "source": [
    "### 3.3. WritingPatternsFeatures Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0f888e-90c1-42f0-9af2-f7b6bee3220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "class NLPFeaturesSpaCy:\n",
    "    \"\"\"\n",
    "    Features that require spaCy NLP: POS counts and syllables.\n",
    "    \"\"\"\n",
    "    def __init__(self, text_col=\"cleaned_text\", model=\"en_core_web_sm\", batch_size=50):\n",
    "        self.text_col = text_col\n",
    "        self.nlp = spacy.load(model, disable=[\"ner\", \"parser\"])  # only need POS\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    @staticmethod\n",
    "    def count_syllables(word):\n",
    "        vowels = \"aeiouy\"\n",
    "        word = word.lower().strip()\n",
    "        count = 0\n",
    "        prev_char_was_vowel = False\n",
    "        for ch in word:\n",
    "            if ch in vowels:\n",
    "                if not prev_char_was_vowel:\n",
    "                    count += 1\n",
    "                prev_char_was_vowel = True\n",
    "            else:\n",
    "                prev_char_was_vowel = False\n",
    "        if word.endswith(\"e\"):\n",
    "            count = max(1, count - 1)\n",
    "        return max(1, count)\n",
    "\n",
    "    def extract_row_features(self, text):\n",
    "        doc = self.nlp(text)\n",
    "        num_adjectives = sum(1 for t in doc if t.pos_ == \"ADJ\")\n",
    "        num_adverbs = sum(1 for t in doc if t.pos_ == \"ADV\")\n",
    "        num_verbs = sum(1 for t in doc if t.pos_ == \"VERB\")\n",
    "        num_determiners = sum(1 for t in doc if t.pos_ == \"DET\")\n",
    "        words = [t.text for t in doc if t.is_alpha]\n",
    "        num_syllables = sum(self.count_syllables(w) for w in words)\n",
    "        return pd.Series({\n",
    "            \"num_adjectives\": num_adjectives,\n",
    "            \"num_adverbs\": num_adverbs,\n",
    "            \"num_verbs\": num_verbs,\n",
    "            \"num_determiners\": num_determiners,\n",
    "            \"num_syllables\": num_syllables\n",
    "        })\n",
    "\n",
    "    def transform(self, spark_df):\n",
    "        # Convert to pandas\n",
    "        pdf = spark_df.select(self.text_col).toPandas()\n",
    "        features_pdf = pdf[self.text_col].apply(self.extract_row_features)\n",
    "        result_pdf = pd.concat([pdf, features_pdf], axis=1)\n",
    "\n",
    "        # Convert back to Spark\n",
    "        spark = SparkSession.builder.getOrCreate()\n",
    "        return spark.createDataFrame(result_pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f8abbf-5084-44e6-9c55-96ca0983dc65",
   "metadata": {},
   "source": [
    "### 3.4. ReadabilityIndices Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebaec79-5f76-4a8b-9a21-b0b735884e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadabilityIndices:\n",
    "    @staticmethod\n",
    "    def extract_features(df, text_col):\n",
    "        schema = StructType([\n",
    "            StructField(\"gunning_fog\", FloatType()),\n",
    "            StructField(\"smog\", FloatType()),\n",
    "            StructField(\"ari\", FloatType())\n",
    "        ])\n",
    "\n",
    "        @pandas_udf(schema)\n",
    "        def udf_readability(text_series: pd.Series) -> pd.DataFrame:\n",
    "            rows = []\n",
    "            for text in text_series.fillna(\"\"):\n",
    "                try:\n",
    "                    gf = textstat.gunning_fog(text)\n",
    "                    smog = textstat.smog_index(text)\n",
    "                    ari = textstat.automated_readability_index(text)\n",
    "                except:\n",
    "                    gf, smog, ari = None, None, None\n",
    "                rows.append({\"gunning_fog\": gf, \"smog\": smog, \"ari\": ari})\n",
    "            return pd.DataFrame(rows)\n",
    "\n",
    "        df = df.withColumn(\"features_struct\", udf_readability(F.col(text_col)))\n",
    "        for field in schema.fieldNames():\n",
    "            df = df.withColumn(field, F.col(f\"features_struct.{field}\"))\n",
    "        return df.drop(\"features_struct\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85448a4-32a2-422e-8fe2-164bac076764",
   "metadata": {},
   "source": [
    "### 3.5. Psycholinguistics Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9bf4e5-4f71-4295-88a4-d15d78af9445",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Psycholinguistics:\n",
    "    @staticmethod\n",
    "    def extract_features(df, text_col, title_col=None):\n",
    "        schema = StructType([\n",
    "            StructField(\"polarity\", FloatType()),\n",
    "            StructField(\"subjectivity\", FloatType()),\n",
    "            StructField(\"title_similarity\", FloatType())\n",
    "        ])\n",
    "\n",
    "        use_title = title_col is not None\n",
    "\n",
    "        @pandas_udf(schema)\n",
    "        def udf_psycho(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "            rows = []\n",
    "            texts = pdf[text_col].fillna(\"\").tolist()\n",
    "            titles = pdf[title_col].fillna(\"\").tolist() if use_title else [None] * len(pdf)\n",
    "            for text, title in zip(texts, titles):\n",
    "                try:\n",
    "                    blob = TextBlob(text)\n",
    "                    polarity = blob.sentiment.polarity\n",
    "                    subjectivity = blob.sentiment.subjectivity\n",
    "                    title_similarity = 0\n",
    "                    if title:\n",
    "                        text_words = set(text.lower().split())\n",
    "                        title_words = set(title.lower().split())\n",
    "                        if text_words and title_words:\n",
    "                            title_similarity = len(text_words & title_words) / len(text_words | title_words)\n",
    "                except:\n",
    "                    polarity, subjectivity, title_similarity = None, None, None\n",
    "                rows.append({\n",
    "                    \"polarity\": polarity,\n",
    "                    \"subjectivity\": subjectivity,\n",
    "                    \"title_similarity\": title_similarity\n",
    "                })\n",
    "            return pd.DataFrame(rows)\n",
    "\n",
    "        select_cols = [text_col] + ([title_col] if use_title else [])\n",
    "        df = df.withColumn(\"features_struct\", udf_psycho(F.struct(*[F.col(c) for c in select_cols])))\n",
    "        for field in schema.fieldNames():\n",
    "            df = df.withColumn(field, F.col(f\"features_struct.{field}\"))\n",
    "        return df.drop(\"features_struct\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf35e8fa",
   "metadata": {},
   "source": [
    "## 4. Configure Spark Environment\n",
    "Using the code snippets from tutorial 1 and 2, set up the Spark environment and configure the Spark Application using SparkConf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcde57ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_home = os.environ.get(\"SPARK_HOME\")\n",
    "\n",
    "if spark_home:\n",
    "    print(f\"SPARK_HOME: {spark_home}\")\n",
    "else:\n",
    "    print(\"SPARK_HOME environement variable is not set.\")\n",
    "\n",
    "os.environ[\"SPARK_HOME\"]= \"/usr/local/lib/python3.10/dist-packages/pyspark\"\n",
    "\n",
    "print (f\"SPARK_HOME is now set to: {os.environ.get('SPARK_HOME')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e920c011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local[*]: run Spark in local mode with as many working processors as logical cores on your machine\n",
    "# If we want Spark to run locally with 'k' worker threads, we can specify as \"local[k]\".\n",
    "master = \"local[*]\"\n",
    "# The `appName` field is a name to be shown on the Spark cluster UI page\n",
    "app_name = \"WELFake Exploratory Data Anlaysis (EDA)\"\n",
    "# Setup configuration parameters for Spark\n",
    "spark_conf = SparkConf().setMaster(master).setAppName(app_name)\n",
    "\n",
    "# Setup SparkSession\n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b156532",
   "metadata": {},
   "source": [
    "## 5. Load datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838b6a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset into Spark dataframe\n",
    "welfake_df = spark.read.csv(\n",
    "    \"data/WELFake_Dataset.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    quote='\"', \n",
    "    multiLine=True, #multilines in text and title data\n",
    "    escape='\"'\n",
    ")\n",
    "\n",
    "# Display sample rows\n",
    "welfake_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27f1052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename first column as index\n",
    "welfake_df = welfake_df.withColumnRenamed(\"_c0\", \"index\")\n",
    "\n",
    "# Show dataframe dimensions\n",
    "num_rows = welfake_df.count()\n",
    "num_cols = len(welfake_df.columns)\n",
    "\n",
    "print(f\"Rows: {num_rows}\")\n",
    "print(f\"Columns: {num_cols}\")\n",
    "\n",
    "#Print the Schema\n",
    "welfake_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f772423",
   "metadata": {},
   "source": [
    "## 6. Remove duplicate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda89b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count original dataset rows\n",
    "original_count = welfake_df.count()\n",
    "\n",
    "# Remove duplicate news articles\n",
    "welfake_df_dedup = welfake_df.dropDuplicates([\"title\", \"text\"])\n",
    "\n",
    "deduped_count = welfake_df_dedup.count()\n",
    "duplicates_removed = original_count - deduped_count\n",
    "\n",
    "print(f\"Original rows: {original_count}\")\n",
    "print(f\"Duplicates removed: {duplicates_removed}\")\n",
    "print(f\"After dataset size: {deduped_count} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e52a567",
   "metadata": {},
   "source": [
    "## 7. Clean title and article texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7258a2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register udf to pyspark\n",
    "clean_text_udf = udf(clean_text, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedf188a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cleaning to title and text\n",
    "welfake_df_clean = welfake_df_dedup.withColumn(\"cleaned_title\", clean_text_udf(\"title\")) \\\n",
    "                       .withColumn(\"cleaned_text\", clean_text_udf(\"text\"))\n",
    "\n",
    "# Preview results\n",
    "welfake_df_clean.select(\"title\", \"cleaned_title\", \"text\", \"cleaned_text\").show(5, truncate=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555ae4e8",
   "metadata": {},
   "source": [
    "## 8. Remove null and empty string values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd93599a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove null or empty string values\n",
    "welfake_df_processed = welfake_df_clean.filter(\n",
    "    (col(\"cleaned_text\").isNotNull()) & \n",
    "    (col(\"cleaned_text\") != \"\") &\n",
    "    (col(\"cleaned_title\").isNotNull()) & \n",
    "    (col(\"cleaned_title\") != \"\") &\n",
    "    (col(\"label\").isNotNull()) \n",
    ")\n",
    "\n",
    "# Count the number of rows with empty values removed\n",
    "clean_count = welfake_df_clean.count()\n",
    "processed_count = welfake_df_processed.count()\n",
    "removed_empty = clean_count - processed_count\n",
    "\n",
    "print(f\"Removed empty text rows: {removed_empty}\")\n",
    "print(f\"After dataset size: {processed_count} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbad9481",
   "metadata": {},
   "source": [
    "## 9. Remove outlier based on text word count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f91fed0",
   "metadata": {},
   "source": [
    "### 9.1. Calculate article text word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993f73f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate text word count\n",
    "welfake_df_wc = welfake_df_processed.withColumn(\"text_wc\", size(split(col(\"cleaned_text\"), \"\\\\s+\")))\n",
    "\n",
    "welfake_df_wc.select(\"cleaned_text\", \"text_wc\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ec9141",
   "metadata": {},
   "source": [
    "### 9.2. Remove outlier based on percentile values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9233136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate key percentiles for text word count\n",
    "percentiles_upper_tail = [0.96, 0.97, 0.98, 0.99]\n",
    "percentiles_lower_tail = [0.01, 0.02, 0.03, 0.04]\n",
    "\n",
    "# Compute percentiles\n",
    "upper_tail_quantiles = welfake_df_wc.approxQuantile(\"text_wc\", percentiles_upper_tail, 0.01)\n",
    "lower_tail_quantiles = welfake_df_wc.approxQuantile(\"text_wc\", percentiles_lower_tail, 0.01)\n",
    "\n",
    "# Show quantile values for analysis\n",
    "print(f\"Upper tail (96% to 99%): {upper_tail_quantiles}\")\n",
    "print(f\"Lower tail (1% to 4%): {lower_tail_quantiles}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ba33a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 2nd and 98th percentiles\n",
    "lower_bound, upper_bound = welfake_df_wc.approxQuantile(\"text_wc\", [0.02, 0.98], 0.01)\n",
    "\n",
    "print(f\"Filter out text_wc < {lower_bound} or > {upper_bound}\\n\")\n",
    "\n",
    "# Filter out values below the 2nd and above the 98th percentiles\n",
    "welfake_df_filtered = welfake_df_wc.filter(\n",
    "    (F.col(\"text_wc\") > lower_bound) & (F.col(\"text_wc\") < upper_bound)\n",
    ")\n",
    "\n",
    "# Count the number of rows with empty values removed\n",
    "outlier_count = welfake_df_filtered.count()\n",
    "removed_outlier = processed_count - outlier_count\n",
    "\n",
    "print(f\"Removed outlier text rows: {removed_outlier}\")\n",
    "print(f\"After dataset size: {outlier_count} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c4212e",
   "metadata": {},
   "source": [
    "## 10. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19dd7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1️⃣ Compute fast PySpark features\n",
    "fast_feat = FeaturesSpark(text_col=\"cleaned_text\")\n",
    "df_fast = fast_feat.transform(welfake_df_filtered)\n",
    "\n",
    "\n",
    "# 3️⃣ Preview\n",
    "df_fast.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3cfc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Assume df is your Spark DataFrame with 'cleaned_text'\n",
    "pdf = welfake_df_filtered.select(\"cleaned_text\").toPandas()\n",
    "\n",
    "# Compute readability features\n",
    "pdf[\"flesch_reading_ease\"] = pdf[\"cleaned_text\"].apply(lambda x: textstat.flesch_reading_ease(x))\n",
    "pdf[\"flesch_kincaid_grade\"] = pdf[\"cleaned_text\"].apply(lambda x: textstat.flesch_kincaid_grade(x))\n",
    "pdf[\"smog_index\"] = pdf[\"cleaned_text\"].apply(lambda x: textstat.smog_index(x))\n",
    "\n",
    "# Convert back to Spark\n",
    "df_readability = spark.createDataFrame(pdf)\n",
    "\n",
    "# Join with original Spark DF if needed\n",
    "df_final = df.join(df_readability, on=\"cleaned_text\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b572364a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a57a9e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
