{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64596a76-df44-4f7c-95ea-9cc9937875c0",
   "metadata": {},
   "source": [
    "## 1. Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7177a3c8-e45f-4bc2-aa40-5e9369463b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "STILL NEED TO ADD VERSIONS (AFTER FILE IS CONFIRMED)\n",
    "\"\"\"\n",
    "%pip install spacy\n",
    "%pip install pyarrow\n",
    "%pip install textblob\n",
    "%pip install textstat\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dba966e-68f2-4df7-a3ef-8f25fbdbe8e1",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbff736-39f8-4787-8ca3-83805762217e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OS environment\n",
    "import os\n",
    "\n",
    "# Import SparkConf class into program\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# Import SparkContext and SparkSession classes\n",
    "from pyspark import SparkContext # Spark\n",
    "from pyspark.sql import SparkSession # Spark SQL\n",
    "\n",
    "# PySpark Data Operations\n",
    "from pyspark.sql.functions import col, size, split, udf\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Regex\n",
    "import re\n",
    "\n",
    "# Numeric operations\n",
    "import numpy as np\n",
    "\n",
    "# Define custom schema (data types) for PySpark Dataframes\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType\n",
    "\n",
    "# spaCy model for natural language processing\n",
    "import spacy\n",
    "\n",
    "# Pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Pscholinguistics\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Readability features\n",
    "import textstat\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc305d0a",
   "metadata": {},
   "source": [
    "## 3. Function and Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1061797-a760-41a2-95bc-0629ff082849",
   "metadata": {},
   "source": [
    "### 3.1. clean_text function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c741c78a-9fb0-4769-bed5-2eed16d8326d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define text cleaning function\n",
    "def clean_text(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    Clean the input text string by removing unwanted elements while keeping useful punctuation.\n",
    "\n",
    "    Steps performed:\n",
    "    - Convert non-ASCII quotes/aprostrophes with ASCII equivalents\n",
    "    - Remove URLs (e.g. http://..., www...)\n",
    "    - Remove Twitter-style mentions (@username) and hashtags (#hashtag)\n",
    "    - Remove HTML entities (e.g. &nbsp;)\n",
    "    - Remove emojis and non-ASCII characters\n",
    "    - Normalize whitespace (convert multiple spaces/tabs/newlines into a single space)\n",
    "    - Trim leading and trailing spaces\n",
    "\n",
    "    Args:\n",
    "        text (str or None): The input text to clean.\n",
    "\n",
    "    Returns:\n",
    "        str: A cleaned version of the input text. If input is None, returns an empty string.\n",
    "    \"\"\"\n",
    "    \n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    \n",
    "    # Replace curly quotes/apostrophes with ASCII equivalents\n",
    "    replacements = {\n",
    "        '“': '\"', '”': '\"',\n",
    "        '‘': \"'\", '’': \"'\"\n",
    "    }\n",
    "    for curly, straight in replacements.items():\n",
    "        text = text.replace(curly, straight)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # Remove mentions and hashtags\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    \n",
    "    # Remove HTMLs\n",
    "    text = re.sub(r'&\\w+;', '', text)\n",
    "    \n",
    "    # Remove emojis and other non-ASCII symbols\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa38dd96-b7e2-4ecb-b69d-771aee837bc8",
   "metadata": {},
   "source": [
    "### 3.2. FeaturesSpark Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb37c0c7-0767-435d-b25e-57e2dc3852a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesSpark:\n",
    "    \"\"\"\n",
    "    Features that can be computed efficiently using PySpark.\n",
    "    \"\"\"\n",
    "    VOWELS = \"aeiouyAEIOUY\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, df, text_col):\n",
    "        txt = F.coalesce(F.col(text_col), F.lit(\"\"))\n",
    "\n",
    "        # Character count\n",
    "        df = df.withColumn(\"num_characters\", F.length(txt))\n",
    "\n",
    "        # Capital letters\n",
    "        df = df.withColumn(\"num_capital_letters\", F.length(F.regexp_replace(txt, r\"[^A-Z]\", \"\")))\n",
    "\n",
    "        # Word count\n",
    "        df = df.withColumn(\"num_words\", F.size(F.split(txt, r\"\\s+\")))\n",
    "\n",
    "        # Sentence count\n",
    "        df = df.withColumn(\"num_sentences\", F.size(F.split(txt, r\"[.!?]+\")))\n",
    "\n",
    "        # Words per sentence\n",
    "        df = df.withColumn(\n",
    "            \"words_per_sentence\", \n",
    "            F.when(F.col(\"num_sentences\") > 0, F.col(\"num_words\") / F.col(\"num_sentences\"))\n",
    "             .otherwise(F.lit(0))\n",
    "        )\n",
    "\n",
    "        # Short sentences (<10 words)\n",
    "        df = df.withColumn(\n",
    "            \"num_short_sentences\", \n",
    "            F.size(F.expr(f\"filter(split({text_col}, '[.!?]+'), x -> size(split(x, ' ')) < 10)\"))\n",
    "        )\n",
    "\n",
    "        # Long sentences (>=20 words)\n",
    "        df = df.withColumn(\n",
    "            \"num_long_sentences\", \n",
    "            F.size(F.expr(f\"filter(split({text_col}, '[.!?]+'), x -> size(split(x, ' ')) >= 20)\"))\n",
    "        )\n",
    "\n",
    "        # Special characters\n",
    "        df = df.withColumn(\"num_special_characters\", F.length(F.regexp_replace(txt, r\"[a-zA-Z0-9\\s]\", \"\")))\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20f1654-e1f3-449b-b2d2-422187f02d76",
   "metadata": {},
   "source": [
    "### 3.3. POSFeatures Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8404993",
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSFeatures:\n",
    "    def __init__(self, model=\"en_core_web_sm\"):\n",
    "        self.model = model\n",
    "\n",
    "    def _count_pos(self, text, pos_tag):\n",
    "        # load model lazily (cached per worker)\n",
    "        if not hasattr(self, \"_nlp\"):\n",
    "            self._nlp = spacy.load(self.model, disable=[\"ner\", \"parser\"])\n",
    "        doc = self._nlp(text)\n",
    "        return sum(1 for token in doc if token.pos_ == pos_tag)\n",
    "\n",
    "    def register_udfs(self, spark):\n",
    "        return {\n",
    "            \"num_nouns\": udf(lambda text: self._count_pos(text, \"NOUN\"), IntegerType()),\n",
    "            \"num_verbs\": udf(lambda text: self._count_pos(text, \"VERB\"), IntegerType()),\n",
    "            \"num_adjectives\": udf(lambda text: self._count_pos(text, \"ADJ\"), IntegerType()),\n",
    "            \"num_adverbs\": udf(lambda text: self._count_pos(text, \"ADV\"), IntegerType()),\n",
    "            \"num_determiners\": udf(lambda text: self._count_pos(text, \"DET\"), IntegerType()),\n",
    "        }\n",
    "\n",
    "    def transform(self, df, text_col):\n",
    "        spark = df.sql_ctx.sparkSession\n",
    "        udfs = self.register_udfs(spark)\n",
    "        for col_name, func in udfs.items():\n",
    "            df = df.withColumn(col_name, func(text_col))\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f8abbf-5084-44e6-9c55-96ca0983dc65",
   "metadata": {},
   "source": [
    "### 3.4. ReadabilityIndices Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebaec79-5f76-4a8b-9a21-b0b735884e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadabilityIndices:\n",
    "    @staticmethod\n",
    "    def extract_features(df, text_col):\n",
    "        # Regular UDFs\n",
    "        gf_udf = F.udf(lambda t: float(textstat.gunning_fog(t)) if t else None, FloatType())\n",
    "        smog_udf = F.udf(lambda t: float(textstat.smog_index(t)) if t else None, FloatType())\n",
    "        ari_udf = F.udf(lambda t: float(textstat.automated_readability_index(t)) if t else None, FloatType())\n",
    "        syllables_udf = F.udf(lambda t: float(textstat.syllable_count(t)) if t else None, FloatType())\n",
    "\n",
    "        return (df\n",
    "            .withColumn(\"gunning_fog\", gf_udf(F.col(text_col)))\n",
    "            .withColumn(\"smog\", smog_udf(F.col(text_col)))\n",
    "            .withColumn(\"ari\", ari_udf(F.col(text_col)))\n",
    "            .withColumn(\"num_syllables\", syllables_udf(F.col(text_col)))\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85448a4-32a2-422e-8fe2-164bac076764",
   "metadata": {},
   "source": [
    "### 3.5. Psycholinguistics Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9bf4e5-4f71-4295-88a4-d15d78af9445",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Psycholinguistics:\n",
    "    @staticmethod\n",
    "    def extract_features(df, text_col, title_col=None):\n",
    "        # polarity\n",
    "        def polarity_udf(text):\n",
    "            if not text:\n",
    "                return None\n",
    "            return float(TextBlob(text).sentiment.polarity)\n",
    "        \n",
    "        # subjectivity\n",
    "        def subjectivity_udf(text):\n",
    "            if not text:\n",
    "                return None\n",
    "            return float(TextBlob(text).sentiment.subjectivity)\n",
    "        \n",
    "        # title similarity\n",
    "        def title_similarity_udf(text, title):\n",
    "            if not text or not title:\n",
    "                return None\n",
    "            text_words = set(text.lower().split())\n",
    "            title_words = set(title.lower().split())\n",
    "            if not text_words or not title_words:\n",
    "                return 0\n",
    "            return float(len(text_words & title_words) / len(text_words | title_words))\n",
    "\n",
    "        df = df.withColumn(\"polarity\", F.udf(polarity_udf, FloatType())(F.col(text_col)))\n",
    "        df = df.withColumn(\"subjectivity\", F.udf(subjectivity_udf, FloatType())(F.col(text_col)))\n",
    "        if title_col:\n",
    "            df = df.withColumn(\"title_similarity\", F.udf(title_similarity_udf, FloatType())(\n",
    "                F.col(text_col), F.col(title_col)\n",
    "            ))\n",
    "        else:\n",
    "            df = df.withColumn(\"title_similarity\", F.lit(None).cast(FloatType()))\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf35e8fa",
   "metadata": {},
   "source": [
    "## 4. Configure Spark Environment\n",
    "Using the code snippets from tutorial 1 and 2, set up the Spark environment and configure the Spark Application using SparkConf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcde57ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_home = os.environ.get(\"SPARK_HOME\")\n",
    "\n",
    "if spark_home:\n",
    "    print(f\"SPARK_HOME: {spark_home}\")\n",
    "else:\n",
    "    print(\"SPARK_HOME environement variable is not set.\")\n",
    "\n",
    "os.environ[\"SPARK_HOME\"]= \"/usr/local/lib/python3.10/dist-packages/pyspark\"\n",
    "\n",
    "print (f\"SPARK_HOME is now set to: {os.environ.get('SPARK_HOME')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e920c011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local[*]: run Spark in local mode with as many working processors as logical cores on your machine\n",
    "# If we want Spark to run locally with 'k' worker threads, we can specify as \"local[k]\".\n",
    "master = \"local[*]\"\n",
    "# The `appName` field is a name to be shown on the Spark cluster UI page\n",
    "app_name = \"WELFake Exploratory Data Anlaysis (EDA)\"\n",
    "# Setup configuration parameters for Spark\n",
    "spark_conf = SparkConf().setMaster(master).setAppName(app_name)\n",
    "\n",
    "# Setup SparkSession\n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b156532",
   "metadata": {},
   "source": [
    "## 5. Load datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838b6a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset into Spark dataframe\n",
    "welfake_df = spark.read.csv(\n",
    "    \"data/WELFake_Dataset.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    quote='\"', \n",
    "    multiLine=True, #multilines in text and title data\n",
    "    escape='\"'\n",
    ")\n",
    "\n",
    "# Display sample rows\n",
    "welfake_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27f1052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename first column as index\n",
    "welfake_df = welfake_df.withColumnRenamed(\"_c0\", \"index\")\n",
    "\n",
    "# Show dataframe dimensions\n",
    "num_rows = welfake_df.count()\n",
    "num_cols = len(welfake_df.columns)\n",
    "\n",
    "print(f\"Rows: {num_rows}\")\n",
    "print(f\"Columns: {num_cols}\")\n",
    "\n",
    "#Print the Schema\n",
    "welfake_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f772423",
   "metadata": {},
   "source": [
    "## 6. Remove duplicate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda89b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count original dataset rows\n",
    "original_count = welfake_df.count()\n",
    "\n",
    "# Remove duplicate news articles\n",
    "welfake_df_dedup = welfake_df.dropDuplicates([\"title\", \"text\"])\n",
    "\n",
    "deduped_count = welfake_df_dedup.count()\n",
    "duplicates_removed = original_count - deduped_count\n",
    "\n",
    "print(f\"Original rows: {original_count}\")\n",
    "print(f\"Duplicates removed: {duplicates_removed}\")\n",
    "print(f\"After dataset size: {deduped_count} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e52a567",
   "metadata": {},
   "source": [
    "## 7. Clean title and article texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7258a2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register udf to pyspark\n",
    "clean_text_udf = udf(clean_text, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedf188a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cleaning to title and text\n",
    "welfake_df_clean = welfake_df_dedup.withColumn(\"cleaned_title\", clean_text_udf(\"title\")) \\\n",
    "                       .withColumn(\"cleaned_text\", clean_text_udf(\"text\"))\n",
    "\n",
    "# Preview results\n",
    "welfake_df_clean.select(\"title\", \"cleaned_title\", \"text\", \"cleaned_text\").show(5, truncate=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555ae4e8",
   "metadata": {},
   "source": [
    "## 8. Remove null and empty string values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd93599a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove null or empty string values\n",
    "welfake_df_processed = welfake_df_clean.filter(\n",
    "    (col(\"cleaned_text\").isNotNull()) & \n",
    "    (col(\"cleaned_text\") != \"\") &\n",
    "    (col(\"cleaned_title\").isNotNull()) & \n",
    "    (col(\"cleaned_title\") != \"\") &\n",
    "    (col(\"label\").isNotNull()) \n",
    ")\n",
    "\n",
    "# Count the number of rows with empty values removed\n",
    "clean_count = welfake_df_clean.count()\n",
    "processed_count = welfake_df_processed.count()\n",
    "removed_empty = clean_count - processed_count\n",
    "\n",
    "print(f\"Removed empty text rows: {removed_empty}\")\n",
    "print(f\"After dataset size: {processed_count} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbad9481",
   "metadata": {},
   "source": [
    "## 9. Remove outlier based on text word count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f91fed0",
   "metadata": {},
   "source": [
    "### 9.1. Calculate article text word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993f73f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate text word count\n",
    "welfake_df_wc = welfake_df_processed.withColumn(\"text_wc\", size(split(col(\"cleaned_text\"), \"\\\\s+\")))\n",
    "\n",
    "welfake_df_wc.select(\"cleaned_text\", \"text_wc\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ec9141",
   "metadata": {},
   "source": [
    "### 9.2. Remove outlier based on percentile values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9233136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate key percentiles for text word count\n",
    "percentiles_upper_tail = [0.96, 0.97, 0.98, 0.99]\n",
    "percentiles_lower_tail = [0.01, 0.02, 0.03, 0.04]\n",
    "\n",
    "# Compute percentiles\n",
    "upper_tail_quantiles = welfake_df_wc.approxQuantile(\"text_wc\", percentiles_upper_tail, 0.01)\n",
    "lower_tail_quantiles = welfake_df_wc.approxQuantile(\"text_wc\", percentiles_lower_tail, 0.01)\n",
    "\n",
    "# Show quantile values for analysis\n",
    "print(f\"Upper tail (96% to 99%): {upper_tail_quantiles}\")\n",
    "print(f\"Lower tail (1% to 4%): {lower_tail_quantiles}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ba33a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 2nd and 98th percentiles\n",
    "lower_bound = 26\n",
    "upper_bound = 1683\n",
    "\n",
    "print(f\"Filter out text_wc < {lower_bound} or > {upper_bound}\\n\")\n",
    "\n",
    "# Filter out values below the 2nd and above the 98th percentiles\n",
    "welfake_df_filtered = welfake_df_wc.filter(\n",
    "    (F.col(\"text_wc\") > lower_bound) & (F.col(\"text_wc\") < upper_bound)\n",
    ")\n",
    "\n",
    "# Count the number of rows with empty values removed\n",
    "outlier_count = welfake_df_filtered.count()\n",
    "removed_outlier = processed_count - outlier_count\n",
    "\n",
    "print(f\"Removed outlier text rows: {removed_outlier}\")\n",
    "print(f\"After dataset size: {outlier_count} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c4212e",
   "metadata": {},
   "source": [
    "## 10. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db71bda",
   "metadata": {},
   "source": [
    "### 10.1. Create quantity feature columns using FeaturesSpark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19dd7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise FeaturesSpark\n",
    "feat_spark = FeaturesSpark()\n",
    "\n",
    "# Create quantity feature columns\n",
    "welfake_df_feat_spark = feat_spark.transform(df=welfake_df_filtered, text_col=\"cleaned_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40bcee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview quanitty feature columns\n",
    "welfake_df_feat_spark.select(\n",
    "    \"cleaned_title\",\n",
    "    \"cleaned_text\",\n",
    "    \"num_characters\",\n",
    "    \"num_special_characters\",\n",
    "    \"num_capital_letters\",\n",
    "    \"num_words\",\n",
    "    \"num_sentences\",\n",
    "    \"words_per_sentence\",\n",
    "    \"num_short_sentences\",\n",
    "    \"num_long_sentences\",\n",
    "    \"label\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bd1739",
   "metadata": {},
   "source": [
    "### 10.2. Create POS feature columns using POSFeatures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3cfc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise POSFeatures\n",
    "pos_features = POSFeatures()\n",
    "\n",
    "# Create POS feature columns\n",
    "welfake_df_pos_feat = pos_features.transform(df=welfake_df_feat_spark, text_col=\"cleaned_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e331221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview POS feature columns\n",
    "welfake_df_pos_feat.select(\n",
    "    \"cleaned_title\",\n",
    "    \"cleaned_text\",\n",
    "    \"num_nouns\",\n",
    "    \"num_verbs\",\n",
    "    \"num_adjectives\",\n",
    "    \"num_adverbs\",\n",
    "    \"num_determiners\",\n",
    "    \"label\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2e9a8b",
   "metadata": {},
   "source": [
    "### 10.3. Create Readability features using ReadabilityIndices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a57a9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise ReadabilityIndices\n",
    "readability = ReadabilityIndices()\n",
    "\n",
    "# Create readability feature columns\n",
    "welfake_df_readability = readability.extract_features(welfake_df_pos_feat, \"cleaned_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42ee50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview readability columns\n",
    "welfake_df_readability.select(\n",
    "    \"cleaned_title\",\n",
    "    \"cleaned_text\",\n",
    "    \"gunning_fog\",\n",
    "    \"smog\",\n",
    "    \"ari\",\n",
    "    \"num_syllables\",\n",
    "    \"label\"\n",
    ").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e374af1",
   "metadata": {},
   "source": [
    "### 10.4. Create psycholinguistics features using Psycholinguistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be84a351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create psycholinguistics feature columns\n",
    "welfake_df_psycho = Psycholinguistics.extract_features(\n",
    "    df=welfake_df_readability,\n",
    "    text_col=\"cleaned_text\",\n",
    "    title_col=\"cleaned_title\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c441448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview psycholinguistics columns\n",
    "welfake_df_psycho.select(\n",
    "    \"cleaned_text\",\n",
    "    \"polarity\",\n",
    "    \"subjectivity\",\n",
    "    \"title_similarity\",\n",
    "    \"label\"\n",
    ").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf09cf2c",
   "metadata": {},
   "source": [
    "### 10.5. Extract engineered features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673df91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature columns for machine learning\n",
    "welfake_df_preprocessed = welfake_df_psycho.select(\n",
    "    \"cleaned_text\",\n",
    "    \"num_characters\",\n",
    "    \"num_special_characters\",\n",
    "    \"num_capital_letters\",\n",
    "    \"num_words\",\n",
    "    \"num_sentences\",\n",
    "    \"words_per_sentence\",\n",
    "    \"num_short_sentences\",\n",
    "    \"num_long_sentences\",\n",
    "    \"num_nouns\",\n",
    "    \"num_verbs\",\n",
    "    \"num_adjectives\",\n",
    "    \"num_adverbs\",\n",
    "    \"num_determiners\",\n",
    "    \"gunning_fog\",\n",
    "    \"smog\",\n",
    "    \"ari\",\n",
    "    \"num_syllables\",\n",
    "    \"polarity\",\n",
    "    \"subjectivity\",\n",
    "    \"title_similarity\",\n",
    "    \"label\"\n",
    ")\n",
    "\n",
    "# Cache results\n",
    "welfake_df_preprocessed.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9f61eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview preprocessed data\n",
    "welfake_df_preprocessed.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
