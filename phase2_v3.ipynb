{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64596a76-df44-4f7c-95ea-9cc9937875c0",
   "metadata": {},
   "source": [
    "## 1. Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7177a3c8-e45f-4bc2-aa40-5e9369463b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spacy in /srv/home/ahar92/.local/lib/python3.10/site-packages (3.8.7)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /srv/home/ahar92/.local/lib/python3.10/site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /srv/home/ahar92/.local/lib/python3.10/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (25.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /srv/home/ahar92/.local/lib/python3.10/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /srv/home/ahar92/.local/lib/python3.10/site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /srv/home/ahar92/.local/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /srv/home/ahar92/.local/lib/python3.10/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /srv/home/ahar92/.local/lib/python3.10/site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /srv/home/ahar92/.local/lib/python3.10/site-packages (from spacy) (2.11.7)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /srv/home/ahar92/.local/lib/python3.10/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /srv/home/ahar92/.local/lib/python3.10/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /srv/home/ahar92/.local/lib/python3.10/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /srv/home/ahar92/.local/lib/python3.10/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.2.6)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy) (59.6.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /srv/home/ahar92/.local/lib/python3.10/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /srv/home/ahar92/.local/lib/python3.10/site-packages (from spacy) (0.16.0)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from spacy) (3.0.3)\n",
      "Requirement already satisfied: language-data>=1.2 in /srv/home/ahar92/.local/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /srv/home/ahar92/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /srv/home/ahar92/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /srv/home/ahar92/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /srv/home/ahar92/.local/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /srv/home/ahar92/.local/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/lib/python3/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.0.3)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /srv/home/ahar92/.local/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /srv/home/ahar92/.local/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.1.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /srv/home/ahar92/.local/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /srv/home/ahar92/.local/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /srv/home/ahar92/.local/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /srv/home/ahar92/.local/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
      "Requirement already satisfied: wrapt in /srv/home/ahar92/.local/lib/python3.10/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /srv/home/ahar92/.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyarrow in /srv/home/ahar92/.local/lib/python3.10/site-packages (21.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: textblob in /srv/home/ahar92/.local/lib/python3.10/site-packages (0.19.0)\n",
      "Requirement already satisfied: nltk>=3.9 in /srv/home/ahar92/.local/lib/python3.10/site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk>=3.9->textblob) (8.0.3)\n",
      "Requirement already satisfied: joblib in /srv/home/ahar92/.local/lib/python3.10/site-packages (from nltk>=3.9->textblob) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /srv/home/ahar92/.local/lib/python3.10/site-packages (from nltk>=3.9->textblob) (2025.7.34)\n",
      "Requirement already satisfied: tqdm in /srv/home/ahar92/.local/lib/python3.10/site-packages (from nltk>=3.9->textblob) (4.67.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: textstat in /srv/home/ahar92/.local/lib/python3.10/site-packages (0.7.8)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from textstat) (59.6.0)\n",
      "Requirement already satisfied: cmudict in /srv/home/ahar92/.local/lib/python3.10/site-packages (from textstat) (1.1.1)\n",
      "Requirement already satisfied: pyphen in /srv/home/ahar92/.local/lib/python3.10/site-packages (from textstat) (0.17.2)\n",
      "Requirement already satisfied: importlib-metadata>=5 in /srv/home/ahar92/.local/lib/python3.10/site-packages (from cmudict->textstat) (8.7.0)\n",
      "Requirement already satisfied: importlib-resources>=5 in /srv/home/ahar92/.local/lib/python3.10/site-packages (from cmudict->textstat) (6.5.2)\n",
      "Requirement already satisfied: zipp>=3.20 in /srv/home/ahar92/.local/lib/python3.10/site-packages (from importlib-metadata>=5->cmudict->textstat) (3.23.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STILL NEED TO ADD VERSIONS (AFTER FILE IS CONFIRMED)\n",
    "\"\"\"\n",
    "%pip install spacy\n",
    "%pip install pyarrow\n",
    "%pip install textblob\n",
    "%pip install textstat\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dba966e-68f2-4df7-a3ef-8f25fbdbe8e1",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afbff736-39f8-4787-8ca3-83805762217e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OS environment\n",
    "import os\n",
    "\n",
    "# Import SparkConf class into program\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# Import SparkContext and SparkSession classes\n",
    "from pyspark import SparkContext # Spark\n",
    "from pyspark.sql import SparkSession # Spark SQL\n",
    "\n",
    "# PySpark Data Operations\n",
    "from pyspark.sql.functions import col, size, split, udf\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Regex\n",
    "import re\n",
    "\n",
    "# Numeric operations\n",
    "import numpy as np\n",
    "\n",
    "# Define custom schema (data types) for PySpark Dataframes\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, StringType\n",
    "\n",
    "# spaCy model for natural language processing\n",
    "import spacy\n",
    "\n",
    "# Pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Pscholinguistics\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Readability features\n",
    "import textstat\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc305d0a",
   "metadata": {},
   "source": [
    "## 3. Function and Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1061797-a760-41a2-95bc-0629ff082849",
   "metadata": {},
   "source": [
    "### 3.1. clean_text function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c741c78a-9fb0-4769-bed5-2eed16d8326d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define text cleaning function\n",
    "def clean_text(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    Clean the input text string by removing unwanted elements while keeping useful punctuation.\n",
    "\n",
    "    Steps performed:\n",
    "    - Convert non-ASCII quotes/aprostrophes with ASCII equivalents\n",
    "    - Remove URLs (e.g. http://..., www...)\n",
    "    - Remove Twitter-style mentions (@username) and hashtags (#hashtag)\n",
    "    - Remove HTML entities (e.g. &nbsp;)\n",
    "    - Remove emojis and non-ASCII characters\n",
    "    - Normalize whitespace (convert multiple spaces/tabs/newlines into a single space)\n",
    "    - Trim leading and trailing spaces\n",
    "\n",
    "    Args:\n",
    "        text (str or None): The input text to clean.\n",
    "\n",
    "    Returns:\n",
    "        str: A cleaned version of the input text. If input is None, returns an empty string.\n",
    "    \"\"\"\n",
    "    \n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    \n",
    "    # Replace curly quotes/apostrophes with ASCII equivalents\n",
    "    replacements = {\n",
    "        '“': '\"', '”': '\"',\n",
    "        '‘': \"'\", '’': \"'\"\n",
    "    }\n",
    "    for curly, straight in replacements.items():\n",
    "        text = text.replace(curly, straight)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # Remove mentions and hashtags\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    \n",
    "    # Remove HTMLs\n",
    "    text = re.sub(r'&\\w+;', '', text)\n",
    "    \n",
    "    # Remove emojis and other non-ASCII symbols\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa38dd96-b7e2-4ecb-b69d-771aee837bc8",
   "metadata": {},
   "source": [
    "### 3.2. FeaturesSpark Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb37c0c7-0767-435d-b25e-57e2dc3852a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "class FeaturesSpark:\n",
    "    \"\"\"\n",
    "    Features that can be computed efficiently using PySpark.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, df, text_col):\n",
    "        txt = F.coalesce(F.col(text_col), F.lit(\"\"))\n",
    "\n",
    "        # Capital letters\n",
    "        df = df.withColumn(\"num_capital_letters\", F.length(F.regexp_replace(txt, r\"[^A-Z]\", \"\")))\n",
    "\n",
    "        # Word count\n",
    "        df = df.withColumn(\"num_words\", F.size(F.split(txt, r\"\\s+\")))\n",
    "\n",
    "        # Sentence count\n",
    "        df = df.withColumn(\"num_sentences\", F.size(F.split(txt, r\"[.!?]+\")))\n",
    "\n",
    "        # Words per sentence\n",
    "        df = df.withColumn(\n",
    "            \"words_per_sentence\", \n",
    "            F.when(F.col(\"num_sentences\") > 0, F.col(\"num_words\") / F.col(\"num_sentences\"))\n",
    "             .otherwise(F.lit(0))\n",
    "        )\n",
    "\n",
    "        # Short sentences (<10 words)\n",
    "        df = df.withColumn(\n",
    "            \"num_short_sentences\", \n",
    "            F.size(F.expr(f\"filter(split({text_col}, '[.!?]+'), x -> size(split(x, ' ')) < 10)\"))\n",
    "        )\n",
    "\n",
    "        # Long sentences (>=20 words)\n",
    "        df = df.withColumn(\n",
    "            \"num_long_sentences\", \n",
    "            F.size(F.expr(f\"filter(split({text_col}, '[.!?]+'), x -> size(split(x, ' ')) >= 20)\"))\n",
    "        )\n",
    "\n",
    "        # Special characters\n",
    "        df = df.withColumn(\"num_special_characters\", F.length(F.regexp_replace(txt, r\"[a-zA-Z0-9\\s]\", \"\")))\n",
    "\n",
    "        # Count articles (a, an, the)\n",
    "        df = df.withColumn(\n",
    "            \"num_articles\",\n",
    "            F.size(F.expr(f\"filter(split(lower({text_col}), '\\\\s+'), x -> x IN ('a', 'an', 'the'))\"))\n",
    "        )\n",
    "\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20f1654-e1f3-449b-b2d2-422187f02d76",
   "metadata": {},
   "source": [
    "### 3.3. POSFeatures Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8404993",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "from pyspark.sql.functions import udf\n",
    "import spacy\n",
    "\n",
    "class POSFeatures:\n",
    "    def __init__(self, model=\"en_core_web_sm\"):\n",
    "        self.model = model\n",
    "\n",
    "    def _count_pos(self, text, pos_tag):\n",
    "        # load model lazily (cached per worker)\n",
    "        if not hasattr(self, \"_nlp\"):\n",
    "            self._nlp = spacy.load(self.model, disable=[\"ner\", \"parser\"])\n",
    "        doc = self._nlp(text)\n",
    "        return sum(1 for token in doc if token.pos_ == pos_tag)\n",
    "\n",
    "    def register_udfs(self, spark):\n",
    "        return {\n",
    "            \"num_verbs\": udf(lambda text: self._count_pos(text, \"VERB\"), IntegerType()),\n",
    "            \"num_adjectives\": udf(lambda text: self._count_pos(text, \"ADJ\"), IntegerType()),\n",
    "            \"num_adverbs\": udf(lambda text: self._count_pos(text, \"ADV\"), IntegerType()),\n",
    "            \"num_determiners\": udf(lambda text: self._count_pos(text, \"DET\"), IntegerType()),\n",
    "        }\n",
    "\n",
    "    def transform(self, df, text_col):\n",
    "        spark = df.sql_ctx.sparkSession\n",
    "        udfs = self.register_udfs(spark)\n",
    "        for col_name, func in udfs.items():\n",
    "            df = df.withColumn(col_name, func(text_col))\n",
    "        \n",
    "        # Calculate rate_adj_adv = (num_adjectives + num_adverbs) / num_words\n",
    "        df = df.withColumn(\n",
    "            \"rate_adj_adv\", \n",
    "            (col(\"num_adjectives\") + col(\"num_adverbs\")) / col(\"num_words\")\n",
    "        )\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f8abbf-5084-44e6-9c55-96ca0983dc65",
   "metadata": {},
   "source": [
    "### 3.4. ReadabilityIndices Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ebaec79-5f76-4a8b-9a21-b0b735884e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadabilityIndices:\n",
    "    @staticmethod\n",
    "    def extract_features(df, text_col):\n",
    "        # Regular UDFs\n",
    "        gf_udf = F.udf(lambda t: float(textstat.gunning_fog(t)) if t else None, FloatType())\n",
    "        smog_udf = F.udf(lambda t: float(textstat.smog_index(t)) if t else None, FloatType())\n",
    "        ari_udf = F.udf(lambda t: float(textstat.automated_readability_index(t)) if t else None, FloatType())\n",
    "        syllables_udf = F.udf(lambda t: float(textstat.syllable_count(t)) if t else None, FloatType())\n",
    "\n",
    "        return (df\n",
    "            .withColumn(\"gunning_fog\", gf_udf(F.col(text_col)))\n",
    "            .withColumn(\"smog\", smog_udf(F.col(text_col)))\n",
    "            .withColumn(\"ari\", ari_udf(F.col(text_col)))\n",
    "            .withColumn(\"num_syllables\", syllables_udf(F.col(text_col)))\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85448a4-32a2-422e-8fe2-164bac076764",
   "metadata": {},
   "source": [
    "### 3.5. Psycholinguistics Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d9bf4e5-4f71-4295-88a4-d15d78af9445",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Psycholinguistics:\n",
    "    @staticmethod\n",
    "    def extract_features(df, text_col, title_col=None):\n",
    "        # polarity\n",
    "        def polarity_udf(text):\n",
    "            if not text:\n",
    "                return None\n",
    "            return float(TextBlob(text).sentiment.polarity)\n",
    "        \n",
    "        # subjectivity\n",
    "        def subjectivity_udf(text):\n",
    "            if not text:\n",
    "                return None\n",
    "            return float(TextBlob(text).sentiment.subjectivity)\n",
    "        \n",
    "        # title similarity\n",
    "        def title_similarity_udf(text, title):\n",
    "            if not text or not title:\n",
    "                return None\n",
    "            text_words = set(text.lower().split())\n",
    "            title_words = set(title.lower().split())\n",
    "            if not text_words or not title_words:\n",
    "                return 0\n",
    "            return float(len(text_words & title_words) / len(text_words | title_words))\n",
    "\n",
    "        df = df.withColumn(\"polarity\", F.udf(polarity_udf, FloatType())(F.col(text_col)))\n",
    "        df = df.withColumn(\"subjectivity\", F.udf(subjectivity_udf, FloatType())(F.col(text_col)))\n",
    "        if title_col:\n",
    "            df = df.withColumn(\"title_similarity\", F.udf(title_similarity_udf, FloatType())(\n",
    "                F.col(text_col), F.col(title_col)\n",
    "            ))\n",
    "        else:\n",
    "            df = df.withColumn(\"title_similarity\", F.lit(None).cast(FloatType()))\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf35e8fa",
   "metadata": {},
   "source": [
    "## 4. Configure Spark Environment\n",
    "Using the code snippets from tutorial 1 and 2, set up the Spark environment and configure the Spark Application using SparkConf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcde57ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK_HOME: /usr/local/lib/python3.8/dist-packages/pyspark\n",
      "SPARK_HOME is now set to: /usr/local/lib/python3.10/dist-packages/pyspark\n"
     ]
    }
   ],
   "source": [
    "spark_home = os.environ.get(\"SPARK_HOME\")\n",
    "\n",
    "if spark_home:\n",
    "    print(f\"SPARK_HOME: {spark_home}\")\n",
    "else:\n",
    "    print(\"SPARK_HOME environement variable is not set.\")\n",
    "\n",
    "os.environ[\"SPARK_HOME\"]= \"/usr/local/lib/python3.10/dist-packages/pyspark\"\n",
    "\n",
    "print (f\"SPARK_HOME is now set to: {os.environ.get('SPARK_HOME')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e920c011",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/lib/python3.10/dist-packages/pyspark/jars/spark-unsafe_2.12-3.0.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "25/08/20 02:36:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# local[*]: run Spark in local mode with as many working processors as logical cores on your machine\n",
    "# If we want Spark to run locally with 'k' worker threads, we can specify as \"local[k]\".\n",
    "master = \"local[*]\"\n",
    "# The `appName` field is a name to be shown on the Spark cluster UI page\n",
    "app_name = \"WELFake Exploratory Data Anlaysis (EDA)\"\n",
    "# Setup configuration parameters for Spark\n",
    "spark_conf = SparkConf().setMaster(master).setAppName(app_name)\n",
    "\n",
    "# Setup SparkSession\n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b156532",
   "metadata": {},
   "source": [
    "## 5. Load datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "838b6a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+-----+\n",
      "|_c0|               title|                text|label|\n",
      "+---+--------------------+--------------------+-----+\n",
      "|  0|LAW ENFORCEMENT O...|No comment is exp...|    1|\n",
      "|  1|                null|Did they post the...|    1|\n",
      "|  2|UNBELIEVABLE! OBA...| Now, most of the...|    1|\n",
      "+---+--------------------+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load dataset into Spark dataframe\n",
    "welfake_df = spark.read.csv(\n",
    "    \"data/WELFake_Dataset.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    quote='\"', \n",
    "    multiLine=True, #multilines in text and title data\n",
    "    escape='\"'\n",
    ")\n",
    "\n",
    "# Display sample rows\n",
    "welfake_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e27f1052",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 72134\n",
      "Columns: 4\n",
      "root\n",
      " |-- index: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Rename first column as index\n",
    "welfake_df = welfake_df.withColumnRenamed(\"_c0\", \"index\")\n",
    "\n",
    "# Show dataframe dimensions\n",
    "num_rows = welfake_df.count()\n",
    "num_cols = len(welfake_df.columns)\n",
    "\n",
    "print(f\"Rows: {num_rows}\")\n",
    "print(f\"Columns: {num_cols}\")\n",
    "\n",
    "#Print the Schema\n",
    "welfake_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f772423",
   "metadata": {},
   "source": [
    "## 6. Remove duplicate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fda89b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:=====================================================> (196 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original rows: 72134\n",
      "Duplicates removed: 8456\n",
      "After dataset size: 63678 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count original dataset rows\n",
    "original_count = welfake_df.count()\n",
    "\n",
    "# Remove duplicate news articles\n",
    "welfake_df_dedup = welfake_df.dropDuplicates([\"title\", \"text\"])\n",
    "\n",
    "deduped_count = welfake_df_dedup.count()\n",
    "duplicates_removed = original_count - deduped_count\n",
    "\n",
    "print(f\"Original rows: {original_count}\")\n",
    "print(f\"Duplicates removed: {duplicates_removed}\")\n",
    "print(f\"After dataset size: {deduped_count} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e52a567",
   "metadata": {},
   "source": [
    "## 7. Clean title and article texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7258a2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register udf to pyspark\n",
    "clean_text_udf = udf(clean_text, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eedf188a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+\n",
      "|                                                                           title|                                                                   cleaned_title|                                                                            text|                                                                    cleaned_text|\n",
      "+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+\n",
      "|Credit Suisse Boss Faces Revolt From Bankers Over Strategy Shift - The New Yo...|Credit Suisse Boss Faces Revolt From Bankers Over Strategy Shift - The New Yo...|When Tidjane Thiam took over at Credit Suisse last July, he laid out a new di...|When Tidjane Thiam took over at Credit Suisse last July, he laid out a new di...|\n",
      "|                      Angry and inspired: Democrats train new wave of candidates|                      Angry and inspired: Democrats train new wave of candidates|ROCKVILLE, Md. (Reuters) - The 100 Democratic women who packed into a suburba...|ROCKVILLE, Md. (Reuters) - The 100 Democratic women who packed into a suburba...|\n",
      "|Russian Economy Minister Arrested For $2 Million Bribe Over Rosneft Deal - Ru...|Russian Economy Minister Arrested For $2 Million Bribe Over Rosneft Deal - Ru...|This post was originally published on this site \n",
      "Update: RUSSIAN PM APPOINTS ...|This post was originally published on this site Update: RUSSIAN PM APPOINTS D...|\n",
      "|HOUSE SPEAKER PAUL RYAN Puts The Party Before The People…TRUMP Gives Awesome ...|HOUSE SPEAKER PAUL RYAN Puts The Party Before The PeopleTRUMP Gives Awesome R...|HOUSE SPEAKER PAUL RYAN HAS SOUR GRAPES ABOUT TRUMP Isn t this about the will...|HOUSE SPEAKER PAUL RYAN HAS SOUR GRAPES ABOUT TRUMP Isn t this about the will...|\n",
      "|(AUDIO) RACIST BLACK PANTHER LEADER: “We’re willing to kill to save a black n...|(AUDIO) RACIST BLACK PANTHER LEADER: \"We're willing to kill to save a black n...|What is it that they don t get? Does anyone even listen to these inciters of ...|What is it that they don t get? Does anyone even listen to these inciters of ...|\n",
      "+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Apply cleaning to title and text\n",
    "welfake_df_clean = welfake_df_dedup.withColumn(\"cleaned_title\", clean_text_udf(\"title\")) \\\n",
    "                       .withColumn(\"cleaned_text\", clean_text_udf(\"text\"))\n",
    "\n",
    "# Preview results\n",
    "welfake_df_clean.select(\"title\", \"cleaned_title\", \"text\", \"cleaned_text\").show(5, truncate=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555ae4e8",
   "metadata": {},
   "source": [
    "## 8. Remove null and empty string values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd93599a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:=====================================================>(197 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed empty text rows: 1186\n",
      "After dataset size: 62492 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Remove null or empty string values\n",
    "welfake_df_processed = welfake_df_clean.filter(\n",
    "    (col(\"cleaned_text\").isNotNull()) & \n",
    "    (col(\"cleaned_text\") != \"\") &\n",
    "    (col(\"cleaned_title\").isNotNull()) & \n",
    "    (col(\"cleaned_title\") != \"\") &\n",
    "    (col(\"label\").isNotNull()) \n",
    ")\n",
    "\n",
    "# Count the number of rows with empty values removed\n",
    "clean_count = welfake_df_clean.count()\n",
    "processed_count = welfake_df_processed.count()\n",
    "removed_empty = clean_count - processed_count\n",
    "\n",
    "print(f\"Removed empty text rows: {removed_empty}\")\n",
    "print(f\"After dataset size: {processed_count} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbad9481",
   "metadata": {},
   "source": [
    "## 9. Remove outlier based on text word count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f91fed0",
   "metadata": {},
   "source": [
    "### 9.1. Calculate article text word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "993f73f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|        cleaned_text|text_wc|\n",
      "+--------------------+-------+\n",
      "|When Tidjane Thia...|   1496|\n",
      "|ROCKVILLE, Md. (R...|    841|\n",
      "|This post was ori...|    754|\n",
      "+--------------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):                                              \n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 642, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n"
     ]
    }
   ],
   "source": [
    "# Calculate text word count\n",
    "welfake_df_wc = welfake_df_processed.withColumn(\"text_wc\", size(split(col(\"cleaned_text\"), \"\\\\s+\")))\n",
    "\n",
    "welfake_df_wc.select(\"cleaned_text\", \"text_wc\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ec9141",
   "metadata": {},
   "source": [
    "### 9.2. Remove outlier based on percentile values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9233136a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:=====================================================>(199 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upper tail (96% to 99%): [1406.0, 1514.0, 1683.0, 24173.0]\n",
      "Lower tail (1% to 4%): [1.0, 26.0, 39.0, 54.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculate key percentiles for text word count\n",
    "percentiles_upper_tail = [0.96, 0.97, 0.98, 0.99]\n",
    "percentiles_lower_tail = [0.01, 0.02, 0.03, 0.04]\n",
    "\n",
    "# Compute percentiles\n",
    "upper_tail_quantiles = welfake_df_wc.approxQuantile(\"text_wc\", percentiles_upper_tail, 0.01)\n",
    "lower_tail_quantiles = welfake_df_wc.approxQuantile(\"text_wc\", percentiles_lower_tail, 0.01)\n",
    "\n",
    "# Show quantile values for analysis\n",
    "print(f\"Upper tail (96% to 99%): {upper_tail_quantiles}\")\n",
    "print(f\"Lower tail (1% to 4%): {lower_tail_quantiles}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6ba33a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter out text_wc < 26.0 or > 1683.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:=====================================================>(198 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed outlier text rows: 2851\n",
      "After dataset size: 59641 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculate 2nd and 98th percentiles\n",
    "lower_bound, upper_bound = welfake_df_wc.approxQuantile(\"text_wc\", [0.02, 0.98], 0.01)\n",
    "\n",
    "print(f\"Filter out text_wc < {lower_bound} or > {upper_bound}\\n\")\n",
    "\n",
    "# Filter out values below the 2nd and above the 98th percentiles\n",
    "welfake_df_filtered = welfake_df_wc.filter(\n",
    "    (F.col(\"text_wc\") > lower_bound) & (F.col(\"text_wc\") < upper_bound)\n",
    ")\n",
    "\n",
    "# Count the number of rows with empty values removed\n",
    "outlier_count = welfake_df_filtered.count()\n",
    "removed_outlier = processed_count - outlier_count\n",
    "\n",
    "print(f\"Removed outlier text rows: {removed_outlier}\")\n",
    "print(f\"After dataset size: {outlier_count} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c4212e",
   "metadata": {},
   "source": [
    "## 10. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db71bda",
   "metadata": {},
   "source": [
    "### 10.1. Create quantity feature columns using FeaturesSpark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a19dd7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise FeaturesSpark\n",
    "feat_spark = FeaturesSpark()\n",
    "\n",
    "# Create quantity feature columns\n",
    "welfake_df_feat_spark = feat_spark.transform(df=welfake_df_filtered, text_col=\"cleaned_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e40bcee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------------------+-------------------+---------+-------------+------------------+-------------------+------------------+------------+-----+\n",
      "|       cleaned_title|        cleaned_text|num_special_characters|num_capital_letters|num_words|num_sentences|words_per_sentence|num_short_sentences|num_long_sentences|num_articles|label|\n",
      "+--------------------+--------------------+----------------------+-------------------+---------+-------------+------------------+-------------------+------------------+------------+-----+\n",
      "|Credit Suisse Bos...|When Tidjane Thia...|                   213|                236|     1496|           91|16.439560439560438|                 23|                38|           0|    0|\n",
      "|Angry and inspire...|ROCKVILLE, Md. (R...|                   143|                124|      841|           35| 24.02857142857143|                  6|                23|           0|    0|\n",
      "|Russian Economy M...|This post was ori...|                   138|                218|      754|           37| 20.37837837837838|                  8|                21|           0|    1|\n",
      "|HOUSE SPEAKER PAU...|HOUSE SPEAKER PAU...|                    14|                 91|      142|            7|20.285714285714285|                  1|                 5|           0|    1|\n",
      "|(AUDIO) RACIST BL...|What is it that t...|                    24|                 50|      204|           13|15.692307692307692|                  5|                 2|           0|    1|\n",
      "+--------------------+--------------------+----------------------+-------------------+---------+-------------+------------------+-------------------+------------------+------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Preview quanitty feature columns\n",
    "welfake_df_feat_spark.select(\n",
    "    \"cleaned_title\",\n",
    "    \"cleaned_text\",\n",
    "    \"num_special_characters\",\n",
    "    \"num_capital_letters\",\n",
    "    \"num_words\",\n",
    "    \"num_sentences\",\n",
    "    \"words_per_sentence\",\n",
    "    \"num_short_sentences\",\n",
    "    \"num_long_sentences\",\n",
    "    \"num_articles\",\n",
    "    \"label\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bd1739",
   "metadata": {},
   "source": [
    "### 10.2. Create POS feature columns using POSFeatures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef3cfc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise POSFeatures\n",
    "pos_features = POSFeatures()\n",
    "\n",
    "# Create POS feature columns\n",
    "welfake_df_pos_feat = pos_features.transform(df=welfake_df_feat_spark, text_col=\"cleaned_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e331221",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+--------------+-----------+---------------+-------------------+-----+\n",
      "|       cleaned_title|        cleaned_text|num_verbs|num_adjectives|num_adverbs|num_determiners|       rate_adj_adv|label|\n",
      "+--------------------+--------------------+---------+--------------+-----------+---------------+-------------------+-----+\n",
      "|Credit Suisse Bos...|When Tidjane Thia...|      157|           126|         60|            143|0.12433155080213903|    0|\n",
      "|Angry and inspire...|ROCKVILLE, Md. (R...|      117|            78|         34|             73|0.13317479191438764|    0|\n",
      "|Russian Economy M...|This post was ori...|       88|            44|         11|             90|0.07294429708222812|    1|\n",
      "|HOUSE SPEAKER PAU...|HOUSE SPEAKER PAU...|       14|             7|         13|             11|0.14084507042253522|    1|\n",
      "|(AUDIO) RACIST BL...|What is it that t...|       31|            11|          2|             16|0.06372549019607843|    1|\n",
      "+--------------------+--------------------+---------+--------------+-----------+---------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Preview POS feature columns\n",
    "welfake_df_pos_feat.select(\n",
    "    \"cleaned_title\",\n",
    "    \"cleaned_text\",\n",
    "    \"num_verbs\",\n",
    "    \"num_adjectives\",\n",
    "    \"num_adverbs\",\n",
    "    \"num_determiners\",\n",
    "     \"rate_adj_adv\",\n",
    "    \"label\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2e9a8b",
   "metadata": {},
   "source": [
    "### 10.3. Create Readability features using ReadabilityIndices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a57a9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise ReadabilityIndices\n",
    "readability = ReadabilityIndices()\n",
    "\n",
    "# Create readability feature columns\n",
    "welfake_df_readability = readability.extract_features(welfake_df_pos_feat, \"cleaned_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c42ee50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------+---------+----------+-------------+-------------------+-----+\n",
      "|       cleaned_title|        cleaned_text|gunning_fog|     smog|       ari|num_syllables|       rate_adj_adv|label|\n",
      "+--------------------+--------------------+-----------+---------+----------+-------------+-------------------+-----+\n",
      "|Credit Suisse Bos...|When Tidjane Thia...|  12.285737|12.139135|10.7932825|       2338.0|0.12433155080213903|    0|\n",
      "|Angry and inspire...|ROCKVILLE, Md. (R...|   16.68025|15.035415| 16.468256|       1388.0|0.13317479191438764|    0|\n",
      "|Russian Economy M...|This post was ori...|  15.220309|14.512509| 13.890554|       1227.0|0.07294429708222812|    1|\n",
      "|HOUSE SPEAKER PAU...|HOUSE SPEAKER PAU...|  10.571428|10.290406|  8.504578|        200.0|0.14084507042253522|    1|\n",
      "|(AUDIO) RACIST BL...|What is it that t...|   9.937255|11.038039|  8.357353|        304.0|0.06372549019607843|    1|\n",
      "+--------------------+--------------------+-----------+---------+----------+-------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Preview readability columns\n",
    "welfake_df_readability.select(\n",
    "    \"cleaned_title\",\n",
    "    \"cleaned_text\",\n",
    "    \"gunning_fog\",\n",
    "    \"smog\",\n",
    "    \"ari\",\n",
    "    \"num_syllables\",\n",
    "    \"rate_adj_adv\",\n",
    "    \"label\"\n",
    ").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e374af1",
   "metadata": {},
   "source": [
    "### 10.4. Create psycholinguistics features using Psycholinguistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be84a351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create psycholinguistics feature columns\n",
    "welfake_df_psycho = Psycholinguistics.extract_features(\n",
    "    df=welfake_df_readability,\n",
    "    text_col=\"cleaned_text\",\n",
    "    title_col=\"cleaned_title\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c441448",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+------------+----------------+-----+\n",
      "|        cleaned_text|    polarity|subjectivity|title_similarity|label|\n",
      "+--------------------+------------+------------+----------------+-----+\n",
      "|When Tidjane Thia...|  0.05515215|  0.40515655|     0.013157895|    0|\n",
      "|ROCKVILLE, Md. (R...|  0.17139433|  0.39709178|     0.014989293|    0|\n",
      "|This post was ori...| 0.056074135|   0.4292704|     0.028423773|    1|\n",
      "|HOUSE SPEAKER PAU...|  0.07604167|  0.40208334|     0.054945055|    1|\n",
      "|What is it that t...|-0.028863637|  0.56545454|     0.059322033|    1|\n",
      "+--------------------+------------+------------+----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Preview psycholinguistics columns\n",
    "welfake_df_psycho.select(\n",
    "    \"cleaned_text\",\n",
    "    \"polarity\",\n",
    "    \"subjectivity\",\n",
    "    \"title_similarity\",\n",
    "    \"label\"\n",
    ").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf09cf2c",
   "metadata": {},
   "source": [
    "### 10.5. Extract engineered features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "673df91a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[cleaned_title: string, cleaned_text: string, num_special_characters: int, num_capital_letters: int, num_words: int, num_sentences: int, words_per_sentence: double, num_short_sentences: int, num_long_sentences: int, num_articles: int, num_verbs: int, num_adjectives: int, num_adverbs: int, rate_adj_adv: double, num_determiners: int, gunning_fog: float, smog: float, ari: float, num_syllables: float, polarity: float, subjectivity: float, title_similarity: float, label: int]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract feature columns for machine learning\n",
    "welfake_df_preprocessed = welfake_df_psycho.select(\n",
    "    \"cleaned_title\",\n",
    "    \"cleaned_text\",\n",
    "    \"num_special_characters\",\n",
    "    \"num_capital_letters\",\n",
    "    \"num_words\",\n",
    "    \"num_sentences\",\n",
    "    \"words_per_sentence\",\n",
    "    \"num_short_sentences\",\n",
    "    \"num_long_sentences\",\n",
    "    \"num_articles\",\n",
    "    \"num_verbs\",\n",
    "    \"num_adjectives\",\n",
    "    \"num_adverbs\",\n",
    "    \"rate_adj_adv\",\n",
    "    \"num_determiners\",\n",
    "    \"gunning_fog\",\n",
    "    \"smog\",\n",
    "    \"ari\",\n",
    "    \"num_syllables\",\n",
    "    \"polarity\",\n",
    "    \"subjectivity\",\n",
    "    \"title_similarity\",\n",
    "    \"label\"\n",
    ")\n",
    "\n",
    "# Cache results\n",
    "welfake_df_preprocessed.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba9f61eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------------------+-------------------+---------+-------------+------------------+-------------------+------------------+------------+---------+--------------+-----------+-------------------+---------------+-----------+---------+----------+-------------+------------+------------+----------------+-----+\n",
      "|       cleaned_title|        cleaned_text|num_special_characters|num_capital_letters|num_words|num_sentences|words_per_sentence|num_short_sentences|num_long_sentences|num_articles|num_verbs|num_adjectives|num_adverbs|       rate_adj_adv|num_determiners|gunning_fog|     smog|       ari|num_syllables|    polarity|subjectivity|title_similarity|label|\n",
      "+--------------------+--------------------+----------------------+-------------------+---------+-------------+------------------+-------------------+------------------+------------+---------+--------------+-----------+-------------------+---------------+-----------+---------+----------+-------------+------------+------------+----------------+-----+\n",
      "|Credit Suisse Bos...|When Tidjane Thia...|                   213|                236|     1496|           91|16.439560439560438|                 23|                38|           0|      157|           126|         60|0.12433155080213903|            143|  12.285737|12.139135|10.7932825|       2338.0|  0.05515215|  0.40515655|     0.013157895|    0|\n",
      "|Angry and inspire...|ROCKVILLE, Md. (R...|                   143|                124|      841|           35| 24.02857142857143|                  6|                23|           0|      117|            78|         34|0.13317479191438764|             73|   16.68025|15.035415| 16.468256|       1388.0|  0.17139433|  0.39709178|     0.014989293|    0|\n",
      "|Russian Economy M...|This post was ori...|                   138|                218|      754|           37| 20.37837837837838|                  8|                21|           0|       88|            44|         11|0.07294429708222812|             90|  15.220309|14.512509| 13.890554|       1227.0| 0.056074135|   0.4292704|     0.028423773|    1|\n",
      "|HOUSE SPEAKER PAU...|HOUSE SPEAKER PAU...|                    14|                 91|      142|            7|20.285714285714285|                  1|                 5|           0|       14|             7|         13|0.14084507042253522|             11|  10.571428|10.290406|  8.504578|        200.0|  0.07604167|  0.40208334|     0.054945055|    1|\n",
      "|(AUDIO) RACIST BL...|What is it that t...|                    24|                 50|      204|           13|15.692307692307692|                  5|                 2|           0|       31|            11|          2|0.06372549019607843|             16|   9.937255|11.038039|  8.357353|        304.0|-0.028863637|  0.56545454|     0.059322033|    1|\n",
      "+--------------------+--------------------+----------------------+-------------------+---------+-------------+------------------+-------------------+------------------+------------+---------+--------------+-----------+-------------------+---------------+-----------+---------+----------+-------------+------------+------------+----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Preview preprocessed data\n",
    "welfake_df_preprocessed.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f2c8df",
   "metadata": {},
   "source": [
    "## 11. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43b7161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb61264b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
