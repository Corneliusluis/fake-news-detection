{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a31b3e0",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c110ad0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/nel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/nel/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import textstat\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "688a7615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text):\n",
    "    try:\n",
    "        tokens = word_tokenize(text)\n",
    "        words = [w for w in tokens if w.isalpha()]\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        pos_tags = pos_tag(words)\n",
    "        \n",
    "        # Writing pattern\n",
    "        num_special_chars = len(re.findall(r'[^a-zA-Z0-9\\s]', text))\n",
    "        num_determinants = sum(1 for w in words if w.lower() in ['the', 'a', 'an'])\n",
    "        num_capital_letters = sum(1 for c in text if c.isupper())\n",
    "        num_short_sentences = sum(1 for s in sentences if len(s.split()) < 10)\n",
    "        num_long_sentences = sum(1 for s in sentences if len(s.split()) > 20)\n",
    "\n",
    "        # Readability indices\n",
    "        gunning_fog = textstat.gunning_fog(text)\n",
    "        smog = textstat.smog_index(text)\n",
    "        ari = textstat.automated_readability_index(text)\n",
    "\n",
    "        # Psycholinguistics\n",
    "        blob = TextBlob(text)\n",
    "        polarity = blob.sentiment.polarity\n",
    "        subjectivity = blob.sentiment.subjectivity\n",
    "        title_similarity = 0  # Optional, you can compute this with cosine or Jaccard if you have 'title'\n",
    "\n",
    "        # Quantity\n",
    "        num_syllables = textstat.syllable_count(text)\n",
    "        num_words = len(words)\n",
    "        num_sentences = len([s for s in sentences if s.strip()])\n",
    "        num_adjectives = sum(1 for _, tag in pos_tags if tag in ['JJ', 'JJR', 'JJS'])\n",
    "        num_adverbs = sum(1 for _, tag in pos_tags if tag in ['RB', 'RBR', 'RBS'])\n",
    "        num_verbs = sum(1 for _, tag in pos_tags if tag.startswith('VB'))\n",
    "        num_articles = sum(1 for w in words if w.lower() in ['a', 'an', 'the'])\n",
    "\n",
    "        rate_adj_adv = (num_adjectives + num_adverbs) / num_words if num_words > 0 else 0\n",
    "        words_per_sentence = num_words / num_sentences if num_sentences > 0 else 0\n",
    "\n",
    "        return pd.Series([\n",
    "            num_special_chars, num_determinants, num_capital_letters, num_short_sentences, num_long_sentences,\n",
    "            gunning_fog, smog, ari,\n",
    "            polarity, title_similarity, subjectivity,\n",
    "            num_syllables, num_words, rate_adj_adv, words_per_sentence,\n",
    "            num_articles, num_verbs, num_sentences, num_adjectives, num_adverbs\n",
    "        ])\n",
    "    except:\n",
    "        return pd.Series([None]*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "086f80bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>cleaned_title</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27473</th>\n",
       "      <td>27540</td>\n",
       "      <td>Catalan pro-independence party PdeCat says wil...</td>\n",
       "      <td>MADRID (Reuters) - Catalan pro-independence pa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52620</th>\n",
       "      <td>4385</td>\n",
       "      <td>Trump says will speak with China's Xi on North...</td>\n",
       "      <td>WASHINGTON (Reuters) - President Donald Trump ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8004</th>\n",
       "      <td>54855</td>\n",
       "      <td>Dan Pfeiffer to leave White House</td>\n",
       "      <td>Dan Pfeiffer, one of President Obama's closest...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19552</th>\n",
       "      <td>31051</td>\n",
       "      <td>Eurofighter jet crashes in Spain, killing pilot</td>\n",
       "      <td>MADRID (Reuters) - A Eurofighter combat jet pl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15749</th>\n",
       "      <td>13354</td>\n",
       "      <td>2nd Night of Trump Protests Brings 29 Arrests ...</td>\n",
       "      <td>Thousands of demonstrators filled the streets ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                                      cleaned_title  \\\n",
       "27473  27540  Catalan pro-independence party PdeCat says wil...   \n",
       "52620   4385  Trump says will speak with China's Xi on North...   \n",
       "8004   54855                  Dan Pfeiffer to leave White House   \n",
       "19552  31051    Eurofighter jet crashes in Spain, killing pilot   \n",
       "15749  13354  2nd Night of Trump Protests Brings 29 Arrests ...   \n",
       "\n",
       "                                            cleaned_text  label  \n",
       "27473  MADRID (Reuters) - Catalan pro-independence pa...      0  \n",
       "52620  WASHINGTON (Reuters) - President Donald Trump ...      0  \n",
       "8004   Dan Pfeiffer, one of President Obama's closest...      0  \n",
       "19552  MADRID (Reuters) - A Eurofighter combat jet pl...      0  \n",
       "15749  Thousands of demonstrators filled the streets ...      0  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('cleaned_welfake.csv')\n",
    "\n",
    "# # Sample 50% from each class\n",
    "# df_sampled = df.groupby('label', group_keys=False).sample(frac=0.5, random_state=42)\n",
    "\n",
    "# Sample 2 rows per class\n",
    "df_sampled = df.groupby('label', group_keys=False).sample(n=100, random_state=42)\n",
    "df_sampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cb2fe7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled.to_csv('sampled_welfake_200.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c47bdf19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2000 entries, 27473 to 33392\n",
      "Data columns (total 24 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   index                2000 non-null   int64  \n",
      " 1   cleaned_title        2000 non-null   object \n",
      " 2   cleaned_text         2000 non-null   object \n",
      " 3   label                2000 non-null   int64  \n",
      " 4   num_special_chars    2000 non-null   float64\n",
      " 5   num_determinants     2000 non-null   float64\n",
      " 6   num_capital_letters  2000 non-null   float64\n",
      " 7   num_short_sentences  2000 non-null   float64\n",
      " 8   num_long_sentences   2000 non-null   float64\n",
      " 9   gunning_fog          2000 non-null   float64\n",
      " 10  smog                 2000 non-null   float64\n",
      " 11  ari                  2000 non-null   float64\n",
      " 12  polarity             2000 non-null   float64\n",
      " 13  title_similarity     2000 non-null   float64\n",
      " 14  subjectivity         2000 non-null   float64\n",
      " 15  num_syllables        2000 non-null   float64\n",
      " 16  num_words            2000 non-null   float64\n",
      " 17  rate_adj_adv         2000 non-null   float64\n",
      " 18  words_per_sentence   2000 non-null   float64\n",
      " 19  num_articles         2000 non-null   float64\n",
      " 20  num_verbs            2000 non-null   float64\n",
      " 21  num_sentences        2000 non-null   float64\n",
      " 22  num_adjectives       2000 non-null   float64\n",
      " 23  num_adverbs          2000 non-null   float64\n",
      "dtypes: float64(20), int64(2), object(2)\n",
      "memory usage: 390.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# Assuming your dataframe is called df and has a 'text' column\n",
    "feature_columns = [\n",
    "    'num_special_chars', 'num_determinants', 'num_capital_letters', 'num_short_sentences', 'num_long_sentences',\n",
    "    'gunning_fog', 'smog', 'ari',\n",
    "    'polarity', 'title_similarity', 'subjectivity',\n",
    "    'num_syllables', 'num_words', 'rate_adj_adv', 'words_per_sentence',\n",
    "    'num_articles', 'num_verbs', 'num_sentences', 'num_adjectives', 'num_adverbs'\n",
    "]\n",
    "\n",
    "# # Apply feature extraction and add prefix to the resulting columns\n",
    "# df_sampled[text_feature_cols := ['text_' + col for col in feature_columns]] = (\n",
    "#     df_sampled['cleaned_text'].apply(extract_features)\n",
    "# )\n",
    "\n",
    "# # Apply feature extraction and add prefix to the resulting columns\n",
    "# df_sampled[title_feature_cols := ['title_' + col for col in feature_columns]] = (\n",
    "#     df_sampled['cleaned_title'].apply(extract_features)\n",
    "# )\n",
    "\n",
    "df_sampled[feature_columns] = df_sampled['cleaned_text'].apply(extract_features)\n",
    "\n",
    "df_sampled.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751e285d",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4305943b",
   "metadata": {},
   "source": [
    "#### Pandas Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44566f53",
   "metadata": {},
   "source": [
    "##### Classic ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dcc0080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    ExtraTreesClassifier,\n",
    "    BaggingClassifier,\n",
    "    AdaBoostClassifier,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "96952905",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    "# text_feature_cols + title_feature_cols\n",
    "    'num_special_chars', 'num_determinants', 'num_capital_letters', 'num_short_sentences', 'num_long_sentences',\n",
    "    'gunning_fog', 'smog', 'ari',\n",
    "    'polarity', 'title_similarity', 'subjectivity',\n",
    "    'num_syllables', 'num_words', 'rate_adj_adv', 'words_per_sentence',\n",
    "    'num_articles', 'num_verbs', 'num_sentences', 'num_adjectives', 'num_adverbs'\n",
    "]\n",
    "\n",
    "X = df_sampled[feature_cols]\n",
    "y = df_sampled['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "83dd0adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Model  Accuracy\n",
      "8          Extra Trees    0.7975\n",
      "7    Gradient Boosting    0.7775\n",
      "6        Random Forest    0.7650\n",
      "1                  SVM    0.7625\n",
      "9  Logistic Regression    0.7550\n",
      "0                  KNN    0.7525\n",
      "4              Bagging    0.7275\n",
      "5             AdaBoost    0.6950\n",
      "3        Decision Tree    0.6375\n",
      "2          Naive Bayes    0.6175\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'SVM': SVC(kernel='linear', probability=True, random_state=42),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Bagging': BaggingClassifier(random_state=42),\n",
    "    'AdaBoost': AdaBoostClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'Extra Trees': ExtraTreesClassifier(n_estimators=100, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    results[name] = {\n",
    "        'accuracy': acc,\n",
    "        'report': classification_report(y_test, y_pred, output_dict=True)\n",
    "    }\n",
    "    # print(f\"âœ… {name} Accuracy: {acc:.4f}\")\n",
    "    # print(classification_report(y_test, y_pred))\n",
    "\n",
    "accuracy_df = pd.DataFrame([\n",
    "    {'Model': name, 'Accuracy': result['accuracy']}\n",
    "    for name, result in results.items()\n",
    "])\n",
    "print(accuracy_df.sort_values(by='Accuracy', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2887db0f",
   "metadata": {},
   "source": [
    "##### WELFake Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8dfa24ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.8916666666666667\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "feature_cols = [\n",
    "# text_feature_cols + title_feature_cols\n",
    "    'num_special_chars', 'num_determinants', 'num_capital_letters', 'num_short_sentences', 'num_long_sentences',\n",
    "    'gunning_fog', 'smog', 'ari',\n",
    "    'polarity', 'title_similarity', 'subjectivity',\n",
    "    'num_syllables', 'num_words', 'rate_adj_adv', 'words_per_sentence',\n",
    "    'num_articles', 'num_verbs', 'num_sentences', 'num_adjectives', 'num_adverbs'\n",
    "]\n",
    "\n",
    "# Example: Define your LFS feature groups (replace with your actual 20 features split into 3 sets)\n",
    "LFS1 = ['num_special_chars', 'num_determinants', 'num_capital_letters', \n",
    "        'gunning_fog', 'polarity', 'num_syllables']\n",
    "LFS2 = ['num_short_sentences', 'smog', 'title_similarity',\n",
    "        'subjectivity', 'num_words', 'rate_adj_adv']\n",
    "LFS3 = ['num_long_sentences', 'ari', 'num_articles', \n",
    "        'num_verbs', 'num_sentences', 'words_per_sentence']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_sampled, df_sampled['label'], test_size=0.3, random_state=42, stratify=df_sampled['label']\n",
    ")\n",
    "\n",
    "# Function to apply CV + LFS\n",
    "def cv_over_lfs(X_train, X_test, lfs_cols):\n",
    "    \"\"\"Apply Count Vectorizer to text + concatenate LFS numeric features.\"\"\"\n",
    "    cv = CountVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "    X_train_cv = cv.fit_transform(X_train['cleaned_text'])\n",
    "    X_test_cv = cv.transform(X_test['cleaned_text'])\n",
    "\n",
    "    # Scale LFS numeric features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_lfs = scaler.fit_transform(X_train[lfs_cols])\n",
    "    X_test_lfs = scaler.transform(X_test[lfs_cols])\n",
    "\n",
    "    # Combine sparse CV with dense LFS\n",
    "    X_train_combined = hstack([X_train_cv, X_train_lfs])\n",
    "    X_test_combined = hstack([X_test_cv, X_test_lfs])\n",
    "    \n",
    "    return X_train_combined, X_test_combined\n",
    "\n",
    "# Generate embedded sets\n",
    "Xtr_LFS1, Xte_LFS1 = cv_over_lfs(X_train, X_test, LFS1)\n",
    "Xtr_LFS2, Xte_LFS2 = cv_over_lfs(X_train, X_test, LFS2)\n",
    "Xtr_LFS3, Xte_LFS3 = cv_over_lfs(X_train, X_test, LFS3)\n",
    "\n",
    "# Define base model (SVM as per WELFake best performer)\n",
    "svm1 = SVC(kernel='linear', probability=True, random_state=42)\n",
    "svm2 = SVC(kernel='linear', probability=True, random_state=42)\n",
    "svm3 = SVC(kernel='linear', probability=True, random_state=42)\n",
    "\n",
    "# Fit each SVM\n",
    "svm1.fit(Xtr_LFS1, y_train)\n",
    "svm2.fit(Xtr_LFS2, y_train)\n",
    "svm3.fit(Xtr_LFS3, y_train)\n",
    "\n",
    "# Stage 1 voting: combine predictions from LFS1, LFS2, LFS3\n",
    "stage1_vote = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('svm1', svm1),\n",
    "        ('svm2', svm2),\n",
    "        ('svm3', svm3)\n",
    "    ],\n",
    "    voting='hard'\n",
    ")\n",
    "stage1_vote.fit(\n",
    "    hstack([Xtr_LFS1, Xtr_LFS2, Xtr_LFS3]),  # Stack features for VotingClassifier fit\n",
    "    y_train\n",
    ")\n",
    "\n",
    "# Stage 1 predictions\n",
    "P6_train = stage1_vote.predict(hstack([Xtr_LFS1, Xtr_LFS2, Xtr_LFS3]))\n",
    "P6_test = stage1_vote.predict(hstack([Xte_LFS1, Xte_LFS2, Xte_LFS3]))\n",
    "\n",
    "# ----- Stage 2: Combine P6 with CV-only and TF-IDF-only -----\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# CV-only on full text\n",
    "cv_full = CountVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "Xtr_cv_full = cv_full.fit_transform(X_train['cleaned_text'])\n",
    "Xte_cv_full = cv_full.transform(X_test['cleaned_text'])\n",
    "\n",
    "# TF-IDF-only on full text\n",
    "tfidf_full = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "Xtr_tfidf_full = tfidf_full.fit_transform(X_train['cleaned_text'])\n",
    "Xte_tfidf_full = tfidf_full.transform(X_test['cleaned_text'])\n",
    "\n",
    "# Final stage voting: P6, CV, TF-IDF\n",
    "final_vote = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('cv_svm', SVC(kernel='linear', probability=True).fit(Xtr_cv_full, y_train)),\n",
    "        ('tfidf_svm', SVC(kernel='linear', probability=True).fit(Xtr_tfidf_full, y_train)),\n",
    "        ('lfs_vote', stage1_vote)\n",
    "    ],\n",
    "    voting='hard'\n",
    ")\n",
    "\n",
    "final_vote.fit(Xtr_cv_full, y_train)  # Fit on one set, predictions from others are internal\n",
    "\n",
    "# Final prediction\n",
    "final_preds = final_vote.predict(Xte_cv_full)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Final Accuracy:\", accuracy_score(y_test, final_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d0e06c",
   "metadata": {},
   "source": [
    "#### Pyspark Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802b942f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/19 20:36:01 WARN Utils: Your hostname, nel-X600-ITX resolves to a loopback address: 127.0.1.1; using 192.168.0.23 instead (on interface wlp4s0)\n",
      "25/08/19 20:36:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/nel/iti5202_big_data/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/jars/spark-unsafe_2.12-3.0.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "25/08/19 20:36:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session started with settings:\n",
      "spark.memory.offHeap.size = 4g\n",
      "spark.driver.memory = 32g\n",
      "spark.memory.fraction = 0.8\n",
      "spark.memory.offHeap.enabled = true\n",
      "spark.executor.memory = 32g\n",
      "spark.executor.cores = 8\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Setup configuration parameters for Spark\n",
    "spark_conf = (\n",
    "    SparkConf()\n",
    "    .setMaster(\"local[12]\")               # safer: 8 threads instead of 16\n",
    "    .setAppName(\"Fake News Detection\")\n",
    "    .set(\"spark.driver.memory\", \"32g\")\n",
    "    .set(\"spark.executor.memory\", \"32g\")\n",
    "    .set(\"spark.executor.cores\", \"8\")\n",
    "    .set(\"spark.sql.shuffle.partitions\", \"128\")\n",
    "    .set(\"spark.memory.fraction\", \"0.8\")\n",
    "    .set(\"spark.memory.offHeap.enabled\", \"true\")\n",
    "    .set(\"spark.memory.offHeap.size\", \"4g\")\n",
    ")\n",
    "\n",
    "# Setup SparkSession\n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(\"Spark Session started with settings:\")\n",
    "for k, v in sc.getConf().getAll():\n",
    "    if \"memory\" in k or \"cores\" in k:\n",
    "        print(f\"{k} = {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc7f1b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+-----+\n",
      "|index|       cleaned_title|        cleaned_text|label|\n",
      "+-----+--------------------+--------------------+-----+\n",
      "|27540|Catalan pro-indep...|MADRID (Reuters) ...|    0|\n",
      "| 4385|Trump says will s...|WASHINGTON (Reute...|    0|\n",
      "|54855|Dan Pfeiffer to l...|Dan Pfeiffer, one...|    0|\n",
      "|31051|Eurofighter jet c...|MADRID (Reuters) ...|    0|\n",
      "|13354|2nd Night of Trum...|Thousands of demo...|    0|\n",
      "+-----+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- index: integer (nullable = true)\n",
      " |-- cleaned_title: string (nullable = true)\n",
      " |-- cleaned_text: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "spark_df = spark.read.csv(\n",
    "    \"sampled_welfake_200.csv\",\n",
    "    # \"cleaned_welfake.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    quote='\"',\n",
    "    escape='\"',\n",
    "    multiLine=True  # needed since your text column contains line breaks\n",
    ")\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "spark_df.show(5)\n",
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58a753fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    0|  100|\n",
      "|    1|  100|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from textblob import TextBlob\n",
    "import textstat\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from pyspark.sql.functions import udf, col, rand, row_number\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, StringType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define schema\n",
    "feature_schema = StructType([\n",
    "    StructField(\"num_special_chars\", IntegerType()),\n",
    "    StructField(\"num_determinants\", IntegerType()),\n",
    "    StructField(\"num_capital_letters\", IntegerType()),\n",
    "    StructField(\"num_short_sentences\", IntegerType()),\n",
    "    StructField(\"num_long_sentences\", IntegerType()),\n",
    "    StructField(\"gunning_fog\", DoubleType()),\n",
    "    StructField(\"smog\", DoubleType()),\n",
    "    StructField(\"ari\", DoubleType()),\n",
    "    StructField(\"polarity\", DoubleType()),\n",
    "    StructField(\"title_similarity\", DoubleType()),\n",
    "    StructField(\"subjectivity\", DoubleType()),\n",
    "    StructField(\"num_syllables\", IntegerType()),\n",
    "    StructField(\"num_words\", IntegerType()),\n",
    "    StructField(\"rate_adj_adv\", DoubleType()),\n",
    "    StructField(\"words_per_sentence\", DoubleType()),\n",
    "    StructField(\"num_articles\", IntegerType()),\n",
    "    StructField(\"num_verbs\", IntegerType()),\n",
    "    StructField(\"num_sentences\", IntegerType()),\n",
    "    StructField(\"num_adjectives\", IntegerType()),\n",
    "    StructField(\"num_adverbs\", IntegerType())\n",
    "])\n",
    "\n",
    "# Feature extraction\n",
    "def extract_features(text):\n",
    "    # try:\n",
    "    tokens = word_tokenize(text)\n",
    "    words = [w for w in tokens if w.isalpha()]\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    pos_tags = pos_tag(words)\n",
    "\n",
    "    num_special_chars = len(re.findall(r'[^a-zA-Z0-9\\s]', text))\n",
    "    num_determinants = sum(1 for w in words if w.lower() in ['the', 'a', 'an'])\n",
    "    num_capital_letters = sum(1 for c in text if c.isupper())\n",
    "    num_short_sent = sum(1 for s in sentences if len(s.split()) < 10)\n",
    "    num_long_sent = sum(1 for s in sentences if len(s.split()) > 20)\n",
    "\n",
    "    gunning_fog = textstat.gunning_fog(text)\n",
    "    smog = textstat.smog_index(text)\n",
    "    ari = textstat.automated_readability_index(text)\n",
    "\n",
    "    blob = TextBlob(text)\n",
    "    polarity = blob.sentiment.polarity\n",
    "    subjectivity = blob.sentiment.subjectivity\n",
    "    title_similarity = 0.0\n",
    "\n",
    "    num_syllables = textstat.syllable_count(text)\n",
    "    num_words = len(words)\n",
    "    num_sentences = len([s for s in sentences if s.strip()])\n",
    "    num_adjectives = sum(1 for _, tag in pos_tags if tag in ['JJ', 'JJR', 'JJS'])\n",
    "    num_adverbs = sum(1 for _, tag in pos_tags if tag in ['RB', 'RBR', 'RBS'])\n",
    "    num_verbs = sum(1 for _, tag in pos_tags if tag.startswith('VB'))\n",
    "    num_articles = sum(1 for w in words if w.lower() in ['a', 'an', 'the'])\n",
    "\n",
    "    rate_adj_adv = (num_adjectives + num_adverbs) / num_words if num_words > 0 else 0\n",
    "    words_per_sent = num_words / num_sentences if num_sentences > 0 else 0\n",
    "\n",
    "    return (\n",
    "        num_special_chars, num_determinants, num_capital_letters, num_short_sent, num_long_sent,\n",
    "        gunning_fog, smog, ari,\n",
    "        polarity, title_similarity, subjectivity,\n",
    "        num_syllables, num_words, rate_adj_adv, words_per_sent,\n",
    "        num_articles, num_verbs, num_sentences, num_adjectives, num_adverbs\n",
    "    )\n",
    "    # except:\n",
    "    #     return (None,) * 20\n",
    "\n",
    "# # --- Stratified sampling with Window (exact N per label) ---\n",
    "# n = 100  # number of rows per class\n",
    "# w = Window.partitionBy(\"label\").orderBy(rand(seed=42))\n",
    "# df_ranked = spark_df.withColumn(\"row_num\", row_number().over(w))\n",
    "# df_sampled = df_ranked.filter(col(\"row_num\") <= n).drop(\"row_num\")\n",
    "\n",
    "# # Verify stratification\n",
    "# df_sampled.groupBy(\"label\").count().show()\n",
    "spark_df.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fbdcbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: integer (nullable = true)\n",
      " |-- cleaned_title: string (nullable = true)\n",
      " |-- cleaned_text: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- features: struct (nullable = true)\n",
      " |    |-- num_special_chars: integer (nullable = true)\n",
      " |    |-- num_determinants: integer (nullable = true)\n",
      " |    |-- num_capital_letters: integer (nullable = true)\n",
      " |    |-- num_short_sentences: integer (nullable = true)\n",
      " |    |-- num_long_sentences: integer (nullable = true)\n",
      " |    |-- gunning_fog: double (nullable = true)\n",
      " |    |-- smog: double (nullable = true)\n",
      " |    |-- ari: double (nullable = true)\n",
      " |    |-- polarity: double (nullable = true)\n",
      " |    |-- title_similarity: double (nullable = true)\n",
      " |    |-- subjectivity: double (nullable = true)\n",
      " |    |-- num_syllables: integer (nullable = true)\n",
      " |    |-- num_words: integer (nullable = true)\n",
      " |    |-- rate_adj_adv: double (nullable = true)\n",
      " |    |-- words_per_sentence: double (nullable = true)\n",
      " |    |-- num_articles: integer (nullable = true)\n",
      " |    |-- num_verbs: integer (nullable = true)\n",
      " |    |-- num_sentences: integer (nullable = true)\n",
      " |    |-- num_adjectives: integer (nullable = true)\n",
      " |    |-- num_adverbs: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register UDF\n",
    "extract_features_udf = udf(extract_features, feature_schema)\n",
    "\n",
    "# Apply feature extraction\n",
    "# df_with_features = df_sampled.withColumn(\"features\", extract_features_udf(\"cleaned_text\"))\n",
    "df_with_features = spark_df.withColumn(\"features\", extract_features_udf(\"cleaned_text\"))\n",
    "\n",
    "df_with_features.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "949e2543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[index: int, cleaned_title: string, cleaned_text: string, label: int, num_special_chars: int, num_determinants: int, num_capital_letters: int, num_short_sentences: int, num_long_sentences: int, gunning_fog: double, smog: double, ari: double, polarity: double, title_similarity: double, subjectivity: double, num_syllables: int, num_words: int, rate_adj_adv: double, words_per_sentence: double, num_articles: int, num_verbs: int, num_sentences: int, num_adjectives: int, num_adverbs: int]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Flatten struct in one go instead of looping with withColumn\n",
    "df_with_features = df_with_features.select(\n",
    "    \"*\",        # keep all existing columns\n",
    "    F.col(\"features.*\")  # expand all fields inside \"features\"\n",
    ").drop(\"features\").na.drop(how=\"any\")  # drop original struct column\n",
    "\n",
    "# # Drop rows with null values\n",
    "# df_with_features = df_with_features.na.drop(how=\"any\")\n",
    "\n",
    "df_with_features.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856f75b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "# import shutil\n",
    "# import os\n",
    "# from pyspark.sql import functions as F\n",
    "\n",
    "# # Select, sanitize, and write a single safely-quoted CSV\n",
    "# temp_path = \"temp_csv_output\"\n",
    "# final_path = \"cleaned_welfake_with_features.csv\"\n",
    "\n",
    "# # Write as a single part with strict quoting\n",
    "# (\n",
    "#     df_with_features\n",
    "#     .coalesce(1)\n",
    "#     .write\n",
    "#     .mode(\"overwrite\")\n",
    "#     .option(\"header\", True)\n",
    "#     .option(\"quote\", '\"')        # use \" as quote char\n",
    "#     .option(\"escape\", '\"')       # escape \" as \"\"\n",
    "#     .option(\"quoteAll\", True)    # quote every field to be extra safe\n",
    "#     .csv(temp_path)\n",
    "# )\n",
    "\n",
    "# # Find the part file inside the temp folder and move it as the final CSV\n",
    "# for file in os.listdir(temp_path):\n",
    "#     if file.startswith(\"part-\") and file.endswith(\".csv\"):\n",
    "#         # remove existing file if present (optional but safer across OSes)\n",
    "#         if os.path.exists(final_path):\n",
    "#             os.remove(final_path)\n",
    "#         shutil.move(os.path.join(temp_path, file), final_path)\n",
    "#         break\n",
    "\n",
    "# # Clean up\n",
    "# shutil.rmtree(temp_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95024da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark UI: http://192.168.0.23:4040\n"
     ]
    }
   ],
   "source": [
    "# Get the Spark UI URL\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")\n",
    "# Usually: http://localhost:4040 or http://driver-ip:4040"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af438dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+-----+-----------------+----------------+-------------------+-------------------+------------------+------------------+------------------+------------------+--------------------+----------------+-------------------+-------------+---------+-------------------+------------------+------------+---------+-------------+--------------+-----------+\n",
      "|index|       cleaned_title|        cleaned_text|label|num_special_chars|num_determinants|num_capital_letters|num_short_sentences|num_long_sentences|       gunning_fog|              smog|               ari|            polarity|title_similarity|       subjectivity|num_syllables|num_words|       rate_adj_adv|words_per_sentence|num_articles|num_verbs|num_sentences|num_adjectives|num_adverbs|\n",
      "+-----+--------------------+--------------------+-----+-----------------+----------------+-------------------+-------------------+------------------+------------------+------------------+------------------+--------------------+----------------+-------------------+-------------+---------+-------------------+------------------+------------+---------+-------------+--------------+-----------+\n",
      "|27540|Catalan pro-indep...|MADRID (Reuters) ...|    0|               18|               6|                 31|                  1|                 2|21.312048192771087|18.511140095513987| 17.37321428571429| -0.2222222222222222|             0.0|0.45555555555555555|          171|       81| 0.1111111111111111|             20.25|           6|       15|            4|             7|          2|\n",
      "| 4385|Trump says will s...|WASHINGTON (Reute...|    0|               14|               2|                 29|                  1|                 1|14.607407407407408|14.554592549557764|12.575818181818185|0.021272727272727266|             0.0| 0.4909090909090909|           90|       54| 0.1111111111111111|              13.5|           2|        7|            4|             4|          2|\n",
      "|54855|Dan Pfeiffer to l...|Dan Pfeiffer, one...|    0|               52|              31|                 65|                  3|                 7|14.438688760806919|13.526455162492045|12.653348853868195| 0.15227478870336014|             0.0| 0.2759348587920016|          529|      342|0.10818713450292397| 20.11764705882353|          31|       59|           17|            26|         11|\n",
      "+-----+--------------------+--------------------+-----+-----------------+----------------+-------------------+-------------------+------------------+------------------+------------------+------------------+--------------------+----------------+-------------------+-------------+---------+-------------------+------------------+------------+---------+-------------+--------------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_with_features.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb2d7b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>cleaned_title</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>label</th>\n",
       "      <th>num_special_chars</th>\n",
       "      <th>num_determinants</th>\n",
       "      <th>num_capital_letters</th>\n",
       "      <th>num_short_sentences</th>\n",
       "      <th>num_long_sentences</th>\n",
       "      <th>gunning_fog</th>\n",
       "      <th>smog</th>\n",
       "      <th>ari</th>\n",
       "      <th>polarity</th>\n",
       "      <th>title_similarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>num_syllables</th>\n",
       "      <th>num_words</th>\n",
       "      <th>rate_adj_adv</th>\n",
       "      <th>words_per_sentence</th>\n",
       "      <th>num_articles</th>\n",
       "      <th>num_verbs</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>num_adjectives</th>\n",
       "      <th>num_adverbs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27540</td>\n",
       "      <td>Catalan pro-independence party PdeCat says wil...</td>\n",
       "      <td>MADRID (Reuters) - Catalan pro-independence pa...</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>21.312048</td>\n",
       "      <td>18.511140</td>\n",
       "      <td>17.373214</td>\n",
       "      <td>-0.222222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.455556</td>\n",
       "      <td>171</td>\n",
       "      <td>81</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>20.250000</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4385</td>\n",
       "      <td>Trump says will speak with China's Xi on North...</td>\n",
       "      <td>WASHINGTON (Reuters) - President Donald Trump ...</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14.607407</td>\n",
       "      <td>14.554593</td>\n",
       "      <td>12.575818</td>\n",
       "      <td>0.021273</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.490909</td>\n",
       "      <td>90</td>\n",
       "      <td>54</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>13.500000</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54855</td>\n",
       "      <td>Dan Pfeiffer to leave White House</td>\n",
       "      <td>Dan Pfeiffer, one of President Obama's closest...</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>31</td>\n",
       "      <td>65</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>14.438689</td>\n",
       "      <td>13.526455</td>\n",
       "      <td>12.653349</td>\n",
       "      <td>0.152275</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.275935</td>\n",
       "      <td>529</td>\n",
       "      <td>342</td>\n",
       "      <td>0.108187</td>\n",
       "      <td>20.117647</td>\n",
       "      <td>31</td>\n",
       "      <td>59</td>\n",
       "      <td>17</td>\n",
       "      <td>26</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31051</td>\n",
       "      <td>Eurofighter jet crashes in Spain, killing pilot</td>\n",
       "      <td>MADRID (Reuters) - A Eurofighter combat jet pl...</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>15.433333</td>\n",
       "      <td>14.554593</td>\n",
       "      <td>14.314227</td>\n",
       "      <td>-0.231250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.404167</td>\n",
       "      <td>158</td>\n",
       "      <td>95</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>23.750000</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13354</td>\n",
       "      <td>2nd Night of Trump Protests Brings 29 Arrests ...</td>\n",
       "      <td>Thousands of demonstrators filled the streets ...</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>31</td>\n",
       "      <td>65</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>11.748434</td>\n",
       "      <td>11.905187</td>\n",
       "      <td>10.869642</td>\n",
       "      <td>0.020588</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.414951</td>\n",
       "      <td>655</td>\n",
       "      <td>402</td>\n",
       "      <td>0.099502</td>\n",
       "      <td>14.888889</td>\n",
       "      <td>31</td>\n",
       "      <td>79</td>\n",
       "      <td>27</td>\n",
       "      <td>28</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                      cleaned_title  \\\n",
       "0  27540  Catalan pro-independence party PdeCat says wil...   \n",
       "1   4385  Trump says will speak with China's Xi on North...   \n",
       "2  54855                  Dan Pfeiffer to leave White House   \n",
       "3  31051    Eurofighter jet crashes in Spain, killing pilot   \n",
       "4  13354  2nd Night of Trump Protests Brings 29 Arrests ...   \n",
       "\n",
       "                                        cleaned_text  label  \\\n",
       "0  MADRID (Reuters) - Catalan pro-independence pa...      0   \n",
       "1  WASHINGTON (Reuters) - President Donald Trump ...      0   \n",
       "2  Dan Pfeiffer, one of President Obama's closest...      0   \n",
       "3  MADRID (Reuters) - A Eurofighter combat jet pl...      0   \n",
       "4  Thousands of demonstrators filled the streets ...      0   \n",
       "\n",
       "   num_special_chars  num_determinants  num_capital_letters  \\\n",
       "0                 18                 6                   31   \n",
       "1                 14                 2                   29   \n",
       "2                 52                31                   65   \n",
       "3                 12                15                   21   \n",
       "4                 84                31                   65   \n",
       "\n",
       "   num_short_sentences  num_long_sentences  gunning_fog       smog        ari  \\\n",
       "0                    1                   2    21.312048  18.511140  17.373214   \n",
       "1                    1                   1    14.607407  14.554593  12.575818   \n",
       "2                    3                   7    14.438689  13.526455  12.653349   \n",
       "3                    1                   3    15.433333  14.554593  14.314227   \n",
       "4                   10                   8    11.748434  11.905187  10.869642   \n",
       "\n",
       "   polarity  title_similarity  subjectivity  num_syllables  num_words  \\\n",
       "0 -0.222222               0.0      0.455556            171         81   \n",
       "1  0.021273               0.0      0.490909             90         54   \n",
       "2  0.152275               0.0      0.275935            529        342   \n",
       "3 -0.231250               0.0      0.404167            158         95   \n",
       "4  0.020588               0.0      0.414951            655        402   \n",
       "\n",
       "   rate_adj_adv  words_per_sentence  num_articles  num_verbs  num_sentences  \\\n",
       "0      0.111111           20.250000             6         15              4   \n",
       "1      0.111111           13.500000             2          7              4   \n",
       "2      0.108187           20.117647            31         59             17   \n",
       "3      0.052632           23.750000            15         19              4   \n",
       "4      0.099502           14.888889            31         79             27   \n",
       "\n",
       "   num_adjectives  num_adverbs  \n",
       "0               7            2  \n",
       "1               4            2  \n",
       "2              26           11  \n",
       "3               5            0  \n",
       "4              28           12  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert to Pandas DataFrame for easier viewing\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.DataFrame(df_with_features.take(5), columns=df_with_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af49fe9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>cleaned_title</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>label</th>\n",
       "      <th>num_special_chars</th>\n",
       "      <th>num_determinants</th>\n",
       "      <th>num_capital_letters</th>\n",
       "      <th>num_short_sentences</th>\n",
       "      <th>num_long_sentences</th>\n",
       "      <th>gunning_fog</th>\n",
       "      <th>smog</th>\n",
       "      <th>ari</th>\n",
       "      <th>polarity</th>\n",
       "      <th>title_similarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>num_syllables</th>\n",
       "      <th>num_words</th>\n",
       "      <th>rate_adj_adv</th>\n",
       "      <th>words_per_sentence</th>\n",
       "      <th>num_articles</th>\n",
       "      <th>num_verbs</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>num_adjectives</th>\n",
       "      <th>num_adverbs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26903</td>\n",
       "      <td>U.S. gun rules heighten tension between police...</td>\n",
       "      <td>WARSAW (Reuters) - President Barack Obama pled...</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>40</td>\n",
       "      <td>73</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>14.814928</td>\n",
       "      <td>13.409020</td>\n",
       "      <td>14.159571</td>\n",
       "      <td>0.004114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.380217</td>\n",
       "      <td>836</td>\n",
       "      <td>538</td>\n",
       "      <td>0.089219</td>\n",
       "      <td>23.391304</td>\n",
       "      <td>40</td>\n",
       "      <td>97</td>\n",
       "      <td>23</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48590</td>\n",
       "      <td>Yemen says Saudi-led coalition to allow commer...</td>\n",
       "      <td>ADEN (Reuters) - The Saudi-led military coalit...</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>25</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>17.268550</td>\n",
       "      <td>15.760457</td>\n",
       "      <td>16.741491</td>\n",
       "      <td>-0.066667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.131250</td>\n",
       "      <td>411</td>\n",
       "      <td>226</td>\n",
       "      <td>0.097345</td>\n",
       "      <td>22.600000</td>\n",
       "      <td>25</td>\n",
       "      <td>34</td>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31781</td>\n",
       "      <td>Yemen air strike kills eight women, two childr...</td>\n",
       "      <td>DUBAI (Reuters) - Eight women and two children...</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>23</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>17.298246</td>\n",
       "      <td>15.470042</td>\n",
       "      <td>17.657417</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.221510</td>\n",
       "      <td>300</td>\n",
       "      <td>183</td>\n",
       "      <td>0.049180</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>23</td>\n",
       "      <td>39</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8921</td>\n",
       "      <td>Donald Trump says he doesn't need a unified GO...</td>\n",
       "      <td>When Donald Trump told ABC's George Stephanopo...</td>\n",
       "      <td>0</td>\n",
       "      <td>180</td>\n",
       "      <td>64</td>\n",
       "      <td>154</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>12.797101</td>\n",
       "      <td>12.709667</td>\n",
       "      <td>10.767404</td>\n",
       "      <td>0.151101</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.472443</td>\n",
       "      <td>1412</td>\n",
       "      <td>903</td>\n",
       "      <td>0.125138</td>\n",
       "      <td>18.428571</td>\n",
       "      <td>64</td>\n",
       "      <td>166</td>\n",
       "      <td>49</td>\n",
       "      <td>66</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14733</td>\n",
       "      <td>All the Clamor? Trump's Palm Beach Neighbors S...</td>\n",
       "      <td>PALM BEACH, Fla. There will be no this weekend...</td>\n",
       "      <td>0</td>\n",
       "      <td>188</td>\n",
       "      <td>79</td>\n",
       "      <td>169</td>\n",
       "      <td>32</td>\n",
       "      <td>20</td>\n",
       "      <td>11.510657</td>\n",
       "      <td>11.789604</td>\n",
       "      <td>10.009782</td>\n",
       "      <td>0.047362</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.414748</td>\n",
       "      <td>1416</td>\n",
       "      <td>890</td>\n",
       "      <td>0.103371</td>\n",
       "      <td>12.535211</td>\n",
       "      <td>79</td>\n",
       "      <td>161</td>\n",
       "      <td>71</td>\n",
       "      <td>65</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                      cleaned_title  \\\n",
       "0  26903  U.S. gun rules heighten tension between police...   \n",
       "1  48590  Yemen says Saudi-led coalition to allow commer...   \n",
       "2  31781  Yemen air strike kills eight women, two childr...   \n",
       "3   8921  Donald Trump says he doesn't need a unified GO...   \n",
       "4  14733  All the Clamor? Trump's Palm Beach Neighbors S...   \n",
       "\n",
       "                                        cleaned_text  label  \\\n",
       "0  WARSAW (Reuters) - President Barack Obama pled...      0   \n",
       "1  ADEN (Reuters) - The Saudi-led military coalit...      0   \n",
       "2  DUBAI (Reuters) - Eight women and two children...      0   \n",
       "3  When Donald Trump told ABC's George Stephanopo...      0   \n",
       "4  PALM BEACH, Fla. There will be no this weekend...      0   \n",
       "\n",
       "   num_special_chars  num_determinants  num_capital_letters  \\\n",
       "0                 67                40                   73   \n",
       "1                 31                25                   61   \n",
       "2                 26                23                   28   \n",
       "3                180                64                  154   \n",
       "4                188                79                  169   \n",
       "\n",
       "   num_short_sentences  num_long_sentences  gunning_fog       smog        ari  \\\n",
       "0                    4                  15    14.814928  13.409020  14.159571   \n",
       "1                    2                   5    17.268550  15.760457  16.741491   \n",
       "2                    1                   6    17.298246  15.470042  17.657417   \n",
       "3                   12                  19    12.797101  12.709667  10.767404   \n",
       "4                   32                  20    11.510657  11.789604  10.009782   \n",
       "\n",
       "   polarity  title_similarity  subjectivity  num_syllables  num_words  \\\n",
       "0  0.004114               0.0      0.380217            836        538   \n",
       "1 -0.066667               0.0      0.131250            411        226   \n",
       "2  0.066667               0.0      0.221510            300        183   \n",
       "3  0.151101               0.0      0.472443           1412        903   \n",
       "4  0.047362               0.0      0.414748           1416        890   \n",
       "\n",
       "   rate_adj_adv  words_per_sentence  num_articles  num_verbs  num_sentences  \\\n",
       "0      0.089219           23.391304            40         97             23   \n",
       "1      0.097345           22.600000            25         34             10   \n",
       "2      0.049180           30.500000            23         39              6   \n",
       "3      0.125138           18.428571            64        166             49   \n",
       "4      0.103371           12.535211            79        161             71   \n",
       "\n",
       "   num_adjectives  num_adverbs  \n",
       "0              32           16  \n",
       "1              19            3  \n",
       "2               6            3  \n",
       "3              66           47  \n",
       "4              65           27  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Flatten struct\n",
    "# for col_name in feature_schema.fieldNames():\n",
    "#     df_with_features = df_with_features.withColumn(col_name, df_with_features[\"features\"][col_name])\n",
    "\n",
    "# df_with_features = df_with_features.drop(\"features\").na.drop(how=\"any\")\n",
    "\n",
    "# Convert to Pandas DataFrame for easier viewing\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.DataFrame(df_with_features.take(5), columns=df_with_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b323dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:=====================================================>  (46 + 2) / 48]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with at least one null/NaN feature: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Define features\n",
    "feature_cols = [\n",
    "    'num_special_chars', 'num_determinants', 'num_capital_letters', 'num_short_sentences', 'num_long_sentences',\n",
    "    'gunning_fog', 'smog', 'ari',\n",
    "    'polarity', 'title_similarity', 'subjectivity',\n",
    "    'num_syllables', 'num_words', 'rate_adj_adv', 'words_per_sentence',\n",
    "    'num_articles', 'num_verbs', 'num_sentences', 'num_adjectives', 'num_adverbs'\n",
    "]\n",
    "\n",
    "# Build condition: check if any feature column is null or NaN\n",
    "condition = None\n",
    "for c in feature_cols:\n",
    "    col_cond = F.col(c).isNull() | F.isnan(c)\n",
    "    condition = col_cond if condition is None else (condition | col_cond)\n",
    "\n",
    "# Count rows with at least one null/NaN\n",
    "null_count = df_with_features.filter(condition).count()\n",
    "print(f\"Rows with at least one null/NaN feature: {null_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4203893e",
   "metadata": {},
   "source": [
    "##### Classic ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce0a5006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 169, Test samples: 31\n",
      "Training Logistic Regression...\n",
      "Logistic Regression - Accuracy: 0.7419, F1: 0.7371, AUC: 0.8361\n",
      "Training Decision Tree...\n",
      "Decision Tree - Accuracy: 0.7419, F1: 0.7371, AUC: 0.7542\n",
      "Training Random Forest...\n",
      "Random Forest - Accuracy: 0.7097, F1: 0.7066, AUC: 0.8824\n",
      "Training Gradient Boosting...\n",
      "Gradient Boosting - Accuracy: 0.7097, F1: 0.7066, AUC: 0.7815\n",
      "Training Naive Bayes...\n",
      "Naive Bayes - Accuracy: 0.7097, F1: 0.6713, AUC: 0.7143\n",
      "\n",
      "================================================================================\n",
      "Model                Accuracy   Precision  Recall     F1         AUC       \n",
      "================================================================================\n",
      "Logistic Regression  0.7419     0.7921     0.7419     0.7371     0.8361    \n",
      "Decision Tree        0.7419     0.7921     0.7419     0.7371     0.7542    \n",
      "Random Forest        0.7097     0.7422     0.7097     0.7066     0.8824    \n",
      "Gradient Boosting    0.7097     0.7422     0.7097     0.7066     0.7815    \n",
      "Naive Bayes          0.7097     0.8102     0.7097     0.6713     0.7143    \n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import (\n",
    "    LogisticRegression, DecisionTreeClassifier, RandomForestClassifier,\n",
    "    GBTClassifier, NaiveBayes\n",
    ")\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "# Define features\n",
    "feature_cols = [\n",
    "    'num_special_chars', 'num_determinants', 'num_capital_letters', 'num_short_sentences', 'num_long_sentences',\n",
    "    'gunning_fog', 'smog', 'ari',\n",
    "    'polarity', 'title_similarity', 'subjectivity',\n",
    "    'num_syllables', 'num_words', 'rate_adj_adv', 'words_per_sentence',\n",
    "    'num_articles', 'num_verbs', 'num_sentences', 'num_adjectives', 'num_adverbs'\n",
    "]\n",
    "\n",
    "# Pre-process data once and persist\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_vec\", handleInvalid=\"skip\")\n",
    "scaler = StandardScaler(inputCol=\"features_vec\", outputCol=\"features\", withStd=True, withMean=True)\n",
    "\n",
    "# Create preprocessing pipeline and apply once\n",
    "preprocessing_pipeline = Pipeline(stages=[assembler, scaler])\n",
    "preprocessing_model = preprocessing_pipeline.fit(df_with_features)\n",
    "processed_df = preprocessing_model.transform(df_with_features)\n",
    "\n",
    "# Train/test split on preprocessed data\n",
    "train_df, test_df = processed_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Persist with default storage level (faster alternative)\n",
    "train_df.cache()\n",
    "test_df.cache()\n",
    "\n",
    "# Force materialization\n",
    "train_count = train_df.count()\n",
    "test_count = test_df.count()\n",
    "print(f\"Train samples: {train_count}, Test samples: {test_count}\")\n",
    "\n",
    "# Optimized models with reduced complexity for speed\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(\n",
    "        featuresCol=\"features\", \n",
    "        labelCol=\"label\", \n",
    "        maxIter=50,  # Reduced from 100\n",
    "        regParam=0.01,\n",
    "        elasticNetParam=0.1\n",
    "    ),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(\n",
    "        featuresCol=\"features\", \n",
    "        labelCol=\"label\",\n",
    "        maxDepth=10,  # Limit depth\n",
    "        maxBins=32    # Reduce bins for speed\n",
    "    ),\n",
    "    \"Random Forest\": RandomForestClassifier(\n",
    "        featuresCol=\"features\", \n",
    "        labelCol=\"label\", \n",
    "        numTrees=50,      # Reduced from 100\n",
    "        maxDepth=10,      # Limit depth\n",
    "        subsamplingRate=0.8,\n",
    "        featureSubsetStrategy=\"sqrt\"\n",
    "    ),\n",
    "    \"Gradient Boosting\": GBTClassifier(\n",
    "        featuresCol=\"features\", \n",
    "        labelCol=\"label\", \n",
    "        maxIter=50,       # Reduced from 100\n",
    "        maxDepth=5,       # Reduced depth\n",
    "        stepSize=0.1\n",
    "    ),\n",
    "    \"Naive Bayes\": NaiveBayes(\n",
    "        featuresCol=\"features\", \n",
    "        labelCol=\"label\", \n",
    "        modelType=\"gaussian\",\n",
    "        smoothing=1.0\n",
    "    )\n",
    "}\n",
    "\n",
    "# Pre-create evaluators\n",
    "bin_eval = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    rawPredictionCol=\"rawPrediction\", \n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "acc_eval = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "f1_eval = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"f1\"\n",
    ")\n",
    "precision_eval = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"weightedPrecision\"\n",
    ")\n",
    "recall_eval = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"weightedRecall\"\n",
    ")\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, clf in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    # Single stage pipeline (preprocessing already done)\n",
    "    pipeline = Pipeline(stages=[clf])\n",
    "    model = pipeline.fit(train_df)\n",
    "    \n",
    "    # Make predictions\n",
    "    preds = model.transform(test_df)\n",
    "    \n",
    "    # Persist predictions for multiple evaluations\n",
    "    preds.cache()\n",
    "    \n",
    "    # Evaluate all metrics on cached predictions\n",
    "    acc = acc_eval.evaluate(preds)\n",
    "    f1 = f1_eval.evaluate(preds)\n",
    "    precision = precision_eval.evaluate(preds)\n",
    "    recall = recall_eval.evaluate(preds)\n",
    "    auc = bin_eval.evaluate(preds)\n",
    "    \n",
    "    results.append((name, acc, precision, recall, f1, auc))\n",
    "    print(f\"{name} - Accuracy: {acc:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}\")\n",
    "    \n",
    "    # Unpersist predictions to free memory\n",
    "    preds.unpersist()\n",
    "\n",
    "# Unpersist datasets\n",
    "train_df.unpersist()\n",
    "test_df.unpersist()\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"{'Model':<20} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1':<10} {'AUC':<10}\")\n",
    "print(\"=\"*80)\n",
    "for result in results:\n",
    "    name, acc, precision, recall, f1, auc = result\n",
    "    print(f\"{name:<20} {acc:<10.4f} {precision:<10.4f} {recall:<10.4f} {f1:<10.4f} {auc:<10.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67884e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>0.792082</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>0.737056</td>\n",
       "      <td>0.836134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>0.792082</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>0.737056</td>\n",
       "      <td>0.754202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.709677</td>\n",
       "      <td>0.742218</td>\n",
       "      <td>0.709677</td>\n",
       "      <td>0.706644</td>\n",
       "      <td>0.882353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.709677</td>\n",
       "      <td>0.742218</td>\n",
       "      <td>0.709677</td>\n",
       "      <td>0.706644</td>\n",
       "      <td>0.781513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.709677</td>\n",
       "      <td>0.810174</td>\n",
       "      <td>0.709677</td>\n",
       "      <td>0.671299</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Accuracy  Precision    Recall        F1       AUC\n",
       "0  Logistic Regression  0.741935   0.792082  0.741935  0.737056  0.836134\n",
       "1        Decision Tree  0.741935   0.792082  0.741935  0.737056  0.754202\n",
       "2        Random Forest  0.709677   0.742218  0.709677  0.706644  0.882353\n",
       "3    Gradient Boosting  0.709677   0.742218  0.709677  0.706644  0.781513\n",
       "4          Naive Bayes  0.709677   0.810174  0.709677  0.671299  0.714286"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert results to Spark DataFrame\n",
    "results_df = spark.createDataFrame(results, [\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"AUC\"]).toPandas()\n",
    "results_df.sort_values('Accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d755e8",
   "metadata": {},
   "source": [
    "##### WELFake Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e429419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark implementation of your 2-stage WELFake-style system\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import (\n",
    "    RegexTokenizer, StopWordsRemover, NGram, CountVectorizer, IDF,\n",
    "    VectorAssembler, StandardScaler\n",
    ")\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Assumptions:\n",
    "# - df (Spark DataFrame) has columns: cleaned_text (string), label (0/1),\n",
    "#   and all 20 LFS numeric columns you listed (already computed).\n",
    "# - Binary classification (0=real, 1=fake). LinearSVC works for binary labels.\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "# ===== 0) Basic hygiene ===============================================================\n",
    "df = df_with_features  # your Spark DataFrame\n",
    "# Cast label to double and ensure no nulls in text/LFS\n",
    "df = (df\n",
    "      .withColumn(\"label\", F.col(\"label\").cast(\"double\"))\n",
    "      .withColumn(\"cleaned_text\", F.coalesce(F.col(\"cleaned_text\"), F.lit(\"\"))))\n",
    "\n",
    "# The 20 features total (adjust names to match your frame if needed)\n",
    "feature_cols = [\n",
    "    'num_special_chars', 'num_determinants', 'num_capital_letters', 'num_short_sentences', 'num_long_sentences',\n",
    "    'gunning_fog', 'smog', 'ari',\n",
    "    'polarity', 'title_similarity', 'subjectivity',\n",
    "    'num_syllables', 'num_words', 'rate_adj_adv', 'words_per_sentence',\n",
    "    'num_articles', 'num_verbs', 'num_sentences', 'num_adjectives', 'num_adverbs'\n",
    "]\n",
    "for c in feature_cols:\n",
    "    df = df.withColumn(c, F.col(c).cast(\"double\"))\n",
    "df = df.fillna(0, subset=feature_cols)\n",
    "\n",
    "# Your LFS splits (can be tweaked)\n",
    "LFS1 = ['num_special_chars', 'num_determinants', 'num_capital_letters', 'gunning_fog', 'polarity', 'num_syllables']\n",
    "LFS2 = ['num_short_sentences', 'smog', 'title_similarity', 'subjectivity', 'num_words', 'rate_adj_adv']\n",
    "LFS3 = ['num_long_sentences', 'ari', 'num_articles', 'num_verbs', 'num_sentences', 'words_per_sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4392116b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 1) Stratified split (approx) ===================================================\n",
    "# Keep class balance similar to scikit-learn's stratify\n",
    "# Create a stable row id to separate train/test\n",
    "df = df.withColumn(\"_row_id\", F.monotonically_increasing_id())\n",
    "\n",
    "test_frac = 0.30\n",
    "label_vals = [r[0] for r in df.select(\"label\").distinct().collect()]\n",
    "fractions = {float(k): test_frac for k in label_vals}\n",
    "test_df = df.sampleBy(\"label\", fractions=fractions, seed=42)\n",
    "train_df = df.join(test_df.select(\"_row_id\"), on=\"_row_id\", how=\"left_anti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77818ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 2) Base text pipeline: tokens -> (uni,bigram) -> CV -> TF-IDF ==================\n",
    "# Tokenize + stopword removal\n",
    "tok = RegexTokenizer(inputCol=\"cleaned_text\", outputCol=\"tokens\", pattern=\"\\\\W+\", toLowercase=True)\n",
    "sw = StopWordsRemover(inputCol=\"tokens\", outputCol=\"tokens_sw\")\n",
    "\n",
    "# 2-grams\n",
    "bi = NGram(n=2, inputCol=\"tokens_sw\", outputCol=\"bigrams\")\n",
    "\n",
    "# CountVectorizer for unigrams and bigrams (vocabSize similar to max_features=5000)\n",
    "cv_uni = CountVectorizer(inputCol=\"tokens_sw\", outputCol=\"tf_uni\", vocabSize=5000, minDF=2)\n",
    "cv_bi  = CountVectorizer(inputCol=\"bigrams\",  outputCol=\"tf_bi\",  vocabSize=5000, minDF=2)\n",
    "\n",
    "# Concatenate uni+bi into \"cv_features\"\n",
    "cv_asm = VectorAssembler(inputCols=[\"tf_uni\", \"tf_bi\"], outputCol=\"cv_features\")\n",
    "\n",
    "# TF-IDF over the same tf (use IDF on the combined term-freqs)\n",
    "idf = IDF(inputCol=\"cv_features\", outputCol=\"tfidf_features\", minDocFreq=2)\n",
    "\n",
    "text_pipe = Pipeline(stages=[tok, sw, bi, cv_uni, cv_bi, cv_asm, idf])\n",
    "text_model = text_pipe.fit(train_df)\n",
    "train_tx = text_model.transform(train_df).cache()\n",
    "test_tx  = text_model.transform(test_df).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22cbaa19",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Column rawPrediction already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Add pred1, pred2, pred3 to test set\u001b[39;00m\n\u001b[1;32m     19\u001b[0m preds \u001b[38;5;241m=\u001b[39m svm1_model\u001b[38;5;241m.\u001b[39mtransform(test_tx)\n\u001b[0;32m---> 20\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43msvm2_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m preds \u001b[38;5;241m=\u001b[39m svm3_model\u001b[38;5;241m.\u001b[39mtransform(preds)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Stage-1 hard vote: majority(pred1, pred2, pred3) â†’ P6\u001b[39;00m\n",
      "File \u001b[0;32m~/iti5202_big_data/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/ml/base.py:170\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_transform(dataset)\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[0;32m~/iti5202_big_data/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/ml/pipeline.py:311\u001b[0m, in \u001b[0;36mPipelineModel._transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset):\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstages:\n\u001b[0;32m--> 311\u001b[0m         dataset \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m~/iti5202_big_data/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/ml/base.py:170\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_transform(dataset)\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[0;32m~/iti5202_big_data/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/ml/wrapper.py:338\u001b[0m, in \u001b[0;36mJavaTransformer._transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset):\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 338\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m, dataset\u001b[38;5;241m.\u001b[39msql_ctx)\n",
      "File \u001b[0;32m~/iti5202_big_data/fake-news-detection/.venv/lib/python3.10/site-packages/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/iti5202_big_data/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/sql/utils.py:137\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    133\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconverted\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Column rawPrediction already exists."
     ]
    }
   ],
   "source": [
    "# ===== 3) Helper to train one SVM on (CV âŠ• scaled LFS) ================================\n",
    "def train_lfs_enabled_svm(lfs_cols, pred_col):\n",
    "    lfs_vec   = VectorAssembler(inputCols=lfs_cols, outputCol=f\"{pred_col}_lfs_vec\")\n",
    "    lfs_scale = StandardScaler(inputCol=f\"{pred_col}_lfs_vec\", outputCol=f\"{pred_col}_lfs_scaled\",\n",
    "                               withStd=True, withMean=True)  # dense numeric -> mean OK\n",
    "    feat_asm  = VectorAssembler(inputCols=[\"cv_features\", f\"{pred_col}_lfs_scaled\"], outputCol=f\"{pred_col}_features\")\n",
    "    svm       = LinearSVC(featuresCol=f\"{pred_col}_features\", labelCol=\"label\",\n",
    "                          predictionCol=pred_col, maxIter=100, regParam=0.1)\n",
    "\n",
    "    pipe = Pipeline(stages=[lfs_vec, lfs_scale, feat_asm, svm])\n",
    "    model = pipe.fit(train_tx)\n",
    "    return model\n",
    "\n",
    "svm1_model = train_lfs_enabled_svm(LFS1, \"pred1\")\n",
    "svm2_model = train_lfs_enabled_svm(LFS2, \"pred2\")\n",
    "svm3_model = train_lfs_enabled_svm(LFS3, \"pred3\")\n",
    "\n",
    "# Add pred1, pred2, pred3 to test set\n",
    "preds = svm1_model.transform(test_tx)\n",
    "preds = svm2_model.transform(preds)\n",
    "preds = svm3_model.transform(preds)\n",
    "\n",
    "# Stage-1 hard vote: majority(pred1, pred2, pred3) â†’ P6\n",
    "preds = preds.withColumn(\n",
    "    \"P6\",\n",
    "    F.when((F.col(\"pred1\") + F.col(\"pred2\") + F.col(\"pred3\")) >= 2, F.lit(1.0)).otherwise(F.lit(0.0))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63524ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 4) CV-only SVM and TF-IDF-only SVM ============================================\n",
    "svm_cv = LinearSVC(featuresCol=\"cv_features\",    labelCol=\"label\", predictionCol=\"pred_cv\",    maxIter=100, regParam=0.1)\n",
    "svm_ti = LinearSVC(featuresCol=\"tfidf_features\", labelCol=\"label\", predictionCol=\"pred_tfidf\", maxIter=100, regParam=0.1)\n",
    "\n",
    "svm_cv_model = svm_cv.fit(train_tx)\n",
    "svm_ti_model = svm_ti.fit(train_tx)\n",
    "\n",
    "preds = svm_cv_model.transform(preds)\n",
    "preds = svm_ti_model.transform(preds)\n",
    "\n",
    "# ===== 5) Final hard vote across {P6, pred_cv, pred_tfidf} ============================\n",
    "preds = preds.withColumn(\n",
    "    \"final_pred\",\n",
    "    F.when((F.col(\"P6\") + F.col(\"pred_cv\") + F.col(\"pred_tfidf\")) >= 2, F.lit(1.0)).otherwise(F.lit(0.0))\n",
    ")\n",
    "\n",
    "# ===== 6) Accuracy ===================================================================\n",
    "acc = preds.select((F.col(\"final_pred\") == F.col(\"label\")).cast(\"double\").alias(\"correct\")) \\\n",
    "           .agg(F.avg(\"correct\").alias(\"accuracy\")) \\\n",
    "           .collect()[0][\"accuracy\"]\n",
    "\n",
    "print(f\"Final Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3baa8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Helper to train one SVM on (CV âŠ• scaled LFS) â€” make columns unique\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "def train_lfs_enabled_svm(lfs_cols, stub):\n",
    "    # stub e.g. \"pred1\", \"pred2\", \"pred3\"\n",
    "    lfs_vec   = VectorAssembler(inputCols=lfs_cols, outputCol=f\"{stub}_lfs_vec\")\n",
    "    lfs_scale = StandardScaler(inputCol=f\"{stub}_lfs_vec\", outputCol=f\"{stub}_lfs_scaled\",\n",
    "                               withStd=True, withMean=True)\n",
    "    feat_asm  = VectorAssembler(inputCols=[\"cv_features\", f\"{stub}_lfs_scaled\"], outputCol=f\"{stub}_features\")\n",
    "\n",
    "    svm = LinearSVC(\n",
    "        featuresCol=f\"{stub}_features\",\n",
    "        labelCol=\"label\",\n",
    "        predictionCol=stub,                  # e.g., pred1\n",
    "        rawPredictionCol=f\"{stub}_raw\",      # e.g., pred1_raw  <<< UNIQUE!\n",
    "        maxIter=100, regParam=0.1\n",
    "    )\n",
    "    return Pipeline(stages=[lfs_vec, lfs_scale, feat_asm, svm]).fit(train_tx)\n",
    "\n",
    "# Train the three LFS-enabled SVMs\n",
    "svm1_model = train_lfs_enabled_svm(LFS1, \"pred1\")\n",
    "svm2_model = train_lfs_enabled_svm(LFS2, \"pred2\")\n",
    "svm3_model = train_lfs_enabled_svm(LFS3, \"pred3\")\n",
    "\n",
    "# Transform sequentially (now no column name clashes)\n",
    "preds = svm1_model.transform(test_tx)\n",
    "preds = svm2_model.transform(preds)\n",
    "preds = svm3_model.transform(preds)\n",
    "\n",
    "# Stage-1 vote\n",
    "from pyspark.sql import functions as F\n",
    "preds = preds.withColumn(\n",
    "    \"P6\", F.when((F.col(\"pred1\") + F.col(\"pred2\") + F.col(\"pred3\")) >= 2.0, 1.0).otherwise(0.0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "073b65b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.8136\n"
     ]
    }
   ],
   "source": [
    "# 4) CV-only and TF-IDF-only SVMs â€” also give unique rawPredictionCol\n",
    "svm_cv = LinearSVC(\n",
    "    featuresCol=\"cv_features\", labelCol=\"label\",\n",
    "    predictionCol=\"pred_cv\", rawPredictionCol=\"raw_cv\",\n",
    "    maxIter=100, regParam=0.1\n",
    ")\n",
    "svm_ti = LinearSVC(\n",
    "    featuresCol=\"tfidf_features\", labelCol=\"label\",\n",
    "    predictionCol=\"pred_tfidf\", rawPredictionCol=\"raw_tfidf\",\n",
    "    maxIter=100, regParam=0.1\n",
    ")\n",
    "\n",
    "svm_cv_model = svm_cv.fit(train_tx)\n",
    "svm_ti_model = svm_ti.fit(train_tx)\n",
    "\n",
    "preds = svm_cv_model.transform(preds)\n",
    "preds = svm_ti_model.transform(preds)\n",
    "\n",
    "# Final vote across {P6, pred_cv, pred_tfidf}\n",
    "preds = preds.withColumn(\n",
    "    \"final_pred\",\n",
    "    F.when((F.col(\"P6\") + F.col(\"pred_cv\") + F.col(\"pred_tfidf\")) >= 2.0, 1.0).otherwise(0.0)\n",
    ")\n",
    "\n",
    "# Accuracy\n",
    "acc = preds.select((F.col(\"final_pred\") == F.col(\"label\")).cast(\"double\").alias(\"correct\")) \\\n",
    "           .agg(F.avg(\"correct\").alias(\"accuracy\")).collect()[0][\"accuracy\"]\n",
    "print(f\"Final Accuracy: {acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "welfake-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
