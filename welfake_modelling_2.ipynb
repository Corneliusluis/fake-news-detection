{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a31b3e0",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c110ad0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/bachtiarherdianto/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/bachtiarherdianto/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import textstat\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "688a7615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text):\n",
    "    try:\n",
    "        tokens = word_tokenize(text)\n",
    "        words = [w for w in tokens if w.isalpha()]\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        pos_tags = pos_tag(words)\n",
    "        \n",
    "        # Writing pattern\n",
    "        num_special_chars = len(re.findall(r'[^a-zA-Z0-9\\s]', text))\n",
    "        num_determinants = sum(1 for w in words if w.lower() in ['the', 'a', 'an'])\n",
    "        num_capital_letters = sum(1 for c in text if c.isupper())\n",
    "        num_short_sentences = sum(1 for s in sentences if len(s.split()) < 10)\n",
    "        num_long_sentences = sum(1 for s in sentences if len(s.split()) > 20)\n",
    "\n",
    "        # Readability indices\n",
    "        gunning_fog = textstat.gunning_fog(text)\n",
    "        smog = textstat.smog_index(text)\n",
    "        ari = textstat.automated_readability_index(text)\n",
    "\n",
    "        # Psycholinguistics\n",
    "        blob = TextBlob(text)\n",
    "        polarity = blob.sentiment.polarity\n",
    "        subjectivity = blob.sentiment.subjectivity\n",
    "        title_similarity = 0  # Optional, you can compute this with cosine or Jaccard if you have 'title'\n",
    "\n",
    "        # Quantity\n",
    "        num_syllables = textstat.syllable_count(text)\n",
    "        num_words = len(words)\n",
    "        num_sentences = len([s for s in sentences if s.strip()])\n",
    "        num_adjectives = sum(1 for _, tag in pos_tags if tag in ['JJ', 'JJR', 'JJS'])\n",
    "        num_adverbs = sum(1 for _, tag in pos_tags if tag in ['RB', 'RBR', 'RBS'])\n",
    "        num_verbs = sum(1 for _, tag in pos_tags if tag.startswith('VB'))\n",
    "        num_articles = sum(1 for w in words if w.lower() in ['a', 'an', 'the'])\n",
    "\n",
    "        rate_adj_adv = (num_adjectives + num_adverbs) / num_words if num_words > 0 else 0\n",
    "        words_per_sentence = num_words / num_sentences if num_sentences > 0 else 0\n",
    "\n",
    "        return pd.Series([\n",
    "            num_special_chars, num_determinants, num_capital_letters, num_short_sentences, num_long_sentences,\n",
    "            gunning_fog, smog, ari,\n",
    "            polarity, title_similarity, subjectivity,\n",
    "            num_syllables, num_words, rate_adj_adv, words_per_sentence,\n",
    "            num_articles, num_verbs, num_sentences, num_adjectives, num_adverbs\n",
    "        ])\n",
    "    except:\n",
    "        return pd.Series([None]*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "086f80bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>cleaned_title</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21354</th>\n",
       "      <td>17322</td>\n",
       "      <td>Trump's choice for U.S. attorney general says ...</td>\n",
       "      <td>WASHINGTON (Reuters) - U.S. President-elect Do...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25012</th>\n",
       "      <td>21871</td>\n",
       "      <td>Alison Wright, Exiled From 'The Americans' (Pe...</td>\n",
       "      <td>It took Alison Wright 34 years to land her fir...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9901</th>\n",
       "      <td>32704</td>\n",
       "      <td>Kurdistan supervisors begin counting votes in ...</td>\n",
       "      <td>ERBIL, Iraq (Reuters) - Voting stations set up...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24177</th>\n",
       "      <td>35894</td>\n",
       "      <td>New Saudi king ascends to the throne as terror...</td>\n",
       "      <td>At 3 a.m. on a cold desert night earlier this ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37552</th>\n",
       "      <td>24368</td>\n",
       "      <td>May shook on gentlemen's agreement on Brexit d...</td>\n",
       "      <td>BRUSSELS (Reuters) - An interim Brexit deal st...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                                      cleaned_title  \\\n",
       "21354  17322  Trump's choice for U.S. attorney general says ...   \n",
       "25012  21871  Alison Wright, Exiled From 'The Americans' (Pe...   \n",
       "9901   32704  Kurdistan supervisors begin counting votes in ...   \n",
       "24177  35894  New Saudi king ascends to the throne as terror...   \n",
       "37552  24368  May shook on gentlemen's agreement on Brexit d...   \n",
       "\n",
       "                                            cleaned_text  label  \n",
       "21354  WASHINGTON (Reuters) - U.S. President-elect Do...      0  \n",
       "25012  It took Alison Wright 34 years to land her fir...      0  \n",
       "9901   ERBIL, Iraq (Reuters) - Voting stations set up...      0  \n",
       "24177  At 3 a.m. on a cold desert night earlier this ...      0  \n",
       "37552  BRUSSELS (Reuters) - An interim Brexit deal st...      0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('cleaned_welfake.csv')\n",
    "\n",
    "# # Sample 50% from each class\n",
    "# df_sampled = df.groupby('label', group_keys=False).sample(frac=0.5, random_state=42)\n",
    "\n",
    "# Sample 2 rows per class\n",
    "df_sampled = df.groupby('label', group_keys=False).sample(n=10000, random_state=42)\n",
    "df_sampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c47bdf19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 467, in main\n",
      "    split_index = read_int(infile)\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 593, in read_int\n",
      "    length = stream.read(4)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 467, in main\n",
      "    split_index = read_int(infile)\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 593, in read_int\n",
      "    length = stream.read(4)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 20\u001b[0m\n\u001b[1;32m      2\u001b[0m feature_columns \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_special_chars\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_determinants\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_capital_letters\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_short_sentences\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_long_sentences\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgunning_fog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mari\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_articles\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_verbs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_sentences\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_adjectives\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_adverbs\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      8\u001b[0m ]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# # Apply feature extraction and add prefix to the resulting columns\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# df_sampled[text_feature_cols := ['text_' + col for col in feature_columns]] = (\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#     df_sampled['cleaned_text'].apply(extract_features)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#     df_sampled['cleaned_title'].apply(extract_features)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m df_sampled[feature_columns] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_sampled\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcleaned_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextract_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m df_sampled\u001b[38;5;241m.\u001b[39minfo()\n",
      "File \u001b[0;32m~/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pandas/core/series.py:4935\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4800\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4801\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4802\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4807\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4808\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4809\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4810\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4811\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4926\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4927\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4928\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4929\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4930\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4931\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4932\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4933\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4935\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pandas/core/apply.py:1422\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1422\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pandas/core/apply.py:1502\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1500\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1502\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1507\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pandas/core/base.py:925\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 925\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mpandas/_libs/lib.pyx:2999\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[3], line 39\u001b[0m, in \u001b[0;36mextract_features\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     37\u001b[0m words \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m w\u001b[38;5;241m.\u001b[39misalpha()]\n\u001b[1;32m     38\u001b[0m sentences \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[.!?]+\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[0;32m---> 39\u001b[0m pos_tags \u001b[38;5;241m=\u001b[39m \u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m num_special_chars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(re\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^a-zA-Z0-9\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]\u001b[39m\u001b[38;5;124m'\u001b[39m, text))\n\u001b[1;32m     42\u001b[0m num_determinants \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124man\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/nltk/tag/__init__.py:169\u001b[0m, in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03mUse NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03mtag the given list of tokens.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;03m:rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    168\u001b[0m tagger \u001b[38;5;241m=\u001b[39m _get_tagger(lang)\n\u001b[0;32m--> 169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_pos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtagset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtagger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/nltk/tag/__init__.py:126\u001b[0m, in \u001b[0;36m_pos_tag\u001b[0;34m(tokens, tagset, tagger, lang)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens: expected a list of strings, got a string\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 126\u001b[0m     tagged_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtagger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tagset:  \u001b[38;5;66;03m# Maps to the specified tagset.\u001b[39;00m\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m lang \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/nltk/tag/perceptron.py:201\u001b[0m, in \u001b[0;36mPerceptronTagger.tag\u001b[0;34m(self, tokens, return_conf, use_tagdict)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tag:\n\u001b[1;32m    200\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_features(i, word, context, prev, prev2)\n\u001b[0;32m--> 201\u001b[0m     tag, conf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_conf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m output\u001b[38;5;241m.\u001b[39mappend((word, tag, conf) \u001b[38;5;28;01mif\u001b[39;00m return_conf \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (word, tag))\n\u001b[1;32m    204\u001b[0m prev2 \u001b[38;5;241m=\u001b[39m prev\n",
      "File \u001b[0;32m~/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/nltk/tag/perceptron.py:83\u001b[0m, in \u001b[0;36mAveragedPerceptron.predict\u001b[0;34m(self, features, return_conf)\u001b[0m\n\u001b[1;32m     81\u001b[0m     weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[feat]\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m label, weight \u001b[38;5;129;01min\u001b[39;00m weights\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 83\u001b[0m         scores[label] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m value \u001b[38;5;241m*\u001b[39m weight\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Do a secondary alphabetic sort, for stability\u001b[39;00m\n\u001b[1;32m     86\u001b[0m best_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m label: (scores[label], label))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Assuming your dataframe is called df and has a 'text' column\n",
    "feature_columns = [\n",
    "    'num_special_chars', 'num_determinants', 'num_capital_letters', 'num_short_sentences', 'num_long_sentences',\n",
    "    'gunning_fog', 'smog', 'ari',\n",
    "    'polarity', 'title_similarity', 'subjectivity',\n",
    "    'num_syllables', 'num_words', 'rate_adj_adv', 'words_per_sentence',\n",
    "    'num_articles', 'num_verbs', 'num_sentences', 'num_adjectives', 'num_adverbs'\n",
    "]\n",
    "\n",
    "# # Apply feature extraction and add prefix to the resulting columns\n",
    "# df_sampled[text_feature_cols := ['text_' + col for col in feature_columns]] = (\n",
    "#     df_sampled['cleaned_text'].apply(extract_features)\n",
    "# )\n",
    "\n",
    "# # Apply feature extraction and add prefix to the resulting columns\n",
    "# df_sampled[title_feature_cols := ['title_' + col for col in feature_columns]] = (\n",
    "#     df_sampled['cleaned_title'].apply(extract_features)\n",
    "# )\n",
    "\n",
    "df_sampled[feature_columns] = df_sampled['cleaned_text'].apply(extract_features)\n",
    "\n",
    "df_sampled.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751e285d",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4305943b",
   "metadata": {},
   "source": [
    "#### Pandas Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44566f53",
   "metadata": {},
   "source": [
    "##### Classic ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcc0080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    ExtraTreesClassifier,\n",
    "    BaggingClassifier,\n",
    "    AdaBoostClassifier,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96952905",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    "# text_feature_cols + title_feature_cols\n",
    "    'num_special_chars', 'num_determinants', 'num_capital_letters', 'num_short_sentences', 'num_long_sentences',\n",
    "    'gunning_fog', 'smog', 'ari',\n",
    "    'polarity', 'title_similarity', 'subjectivity',\n",
    "    'num_syllables', 'num_words', 'rate_adj_adv', 'words_per_sentence',\n",
    "    'num_articles', 'num_verbs', 'num_sentences', 'num_adjectives', 'num_adverbs'\n",
    "]\n",
    "\n",
    "X = df_sampled[feature_cols]\n",
    "y = df_sampled['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83dd0adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Model  Accuracy\n",
      "1                  SVM     0.825\n",
      "9  Logistic Regression     0.825\n",
      "4              Bagging     0.725\n",
      "6        Random Forest     0.725\n",
      "8          Extra Trees     0.725\n",
      "0                  KNN     0.675\n",
      "5             AdaBoost     0.675\n",
      "2          Naive Bayes     0.650\n",
      "3        Decision Tree     0.650\n",
      "7    Gradient Boosting     0.650\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'SVM': SVC(kernel='linear', probability=True, random_state=42),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Bagging': BaggingClassifier(random_state=42),\n",
    "    'AdaBoost': AdaBoostClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'Extra Trees': ExtraTreesClassifier(n_estimators=100, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    results[name] = {\n",
    "        'accuracy': acc,\n",
    "        'report': classification_report(y_test, y_pred, output_dict=True)\n",
    "    }\n",
    "    # print(f\"✅ {name} Accuracy: {acc:.4f}\")\n",
    "    # print(classification_report(y_test, y_pred))\n",
    "\n",
    "accuracy_df = pd.DataFrame([\n",
    "    {'Model': name, 'Accuracy': result['accuracy']}\n",
    "    for name, result in results.items()\n",
    "])\n",
    "print(accuracy_df.sort_values(by='Accuracy', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2887db0f",
   "metadata": {},
   "source": [
    "##### WELFake Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dfa24ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.8666666666666667\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "feature_cols = [\n",
    "# text_feature_cols + title_feature_cols\n",
    "    'num_special_chars', 'num_determinants', 'num_capital_letters', 'num_short_sentences', 'num_long_sentences',\n",
    "    'gunning_fog', 'smog', 'ari',\n",
    "    'polarity', 'title_similarity', 'subjectivity',\n",
    "    'num_syllables', 'num_words', 'rate_adj_adv', 'words_per_sentence',\n",
    "    'num_articles', 'num_verbs', 'num_sentences', 'num_adjectives', 'num_adverbs'\n",
    "]\n",
    "\n",
    "# Example: Define your LFS feature groups (replace with your actual 20 features split into 3 sets)\n",
    "LFS1 = ['num_special_chars', 'num_determinants', 'num_capital_letters', \n",
    "        'gunning_fog', 'polarity', 'num_syllables']\n",
    "LFS2 = ['num_short_sentences', 'smog', 'title_similarity',\n",
    "        'subjectivity', 'num_words', 'rate_adj_adv']\n",
    "LFS3 = ['num_long_sentences', 'ari', 'num_articles', \n",
    "        'num_verbs', 'num_sentences', 'words_per_sentence']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_sampled, df_sampled['label'], test_size=0.3, random_state=42, stratify=df_sampled['label']\n",
    ")\n",
    "\n",
    "# Function to apply CV + LFS\n",
    "def cv_over_lfs(X_train, X_test, lfs_cols):\n",
    "    \"\"\"Apply Count Vectorizer to text + concatenate LFS numeric features.\"\"\"\n",
    "    cv = CountVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "    X_train_cv = cv.fit_transform(X_train['cleaned_text'])\n",
    "    X_test_cv = cv.transform(X_test['cleaned_text'])\n",
    "\n",
    "    # Scale LFS numeric features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_lfs = scaler.fit_transform(X_train[lfs_cols])\n",
    "    X_test_lfs = scaler.transform(X_test[lfs_cols])\n",
    "\n",
    "    # Combine sparse CV with dense LFS\n",
    "    X_train_combined = hstack([X_train_cv, X_train_lfs])\n",
    "    X_test_combined = hstack([X_test_cv, X_test_lfs])\n",
    "    \n",
    "    return X_train_combined, X_test_combined\n",
    "\n",
    "# Generate embedded sets\n",
    "Xtr_LFS1, Xte_LFS1 = cv_over_lfs(X_train, X_test, LFS1)\n",
    "Xtr_LFS2, Xte_LFS2 = cv_over_lfs(X_train, X_test, LFS2)\n",
    "Xtr_LFS3, Xte_LFS3 = cv_over_lfs(X_train, X_test, LFS3)\n",
    "\n",
    "# Define base model (SVM as per WELFake best performer)\n",
    "svm1 = SVC(kernel='linear', probability=True, random_state=42)\n",
    "svm2 = SVC(kernel='linear', probability=True, random_state=42)\n",
    "svm3 = SVC(kernel='linear', probability=True, random_state=42)\n",
    "\n",
    "# Fit each SVM\n",
    "svm1.fit(Xtr_LFS1, y_train)\n",
    "svm2.fit(Xtr_LFS2, y_train)\n",
    "svm3.fit(Xtr_LFS3, y_train)\n",
    "\n",
    "# Stage 1 voting: combine predictions from LFS1, LFS2, LFS3\n",
    "stage1_vote = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('svm1', svm1),\n",
    "        ('svm2', svm2),\n",
    "        ('svm3', svm3)\n",
    "    ],\n",
    "    voting='hard'\n",
    ")\n",
    "stage1_vote.fit(\n",
    "    hstack([Xtr_LFS1, Xtr_LFS2, Xtr_LFS3]),  # Stack features for VotingClassifier fit\n",
    "    y_train\n",
    ")\n",
    "\n",
    "# Stage 1 predictions\n",
    "P6_train = stage1_vote.predict(hstack([Xtr_LFS1, Xtr_LFS2, Xtr_LFS3]))\n",
    "P6_test = stage1_vote.predict(hstack([Xte_LFS1, Xte_LFS2, Xte_LFS3]))\n",
    "\n",
    "# ----- Stage 2: Combine P6 with CV-only and TF-IDF-only -----\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# CV-only on full text\n",
    "cv_full = CountVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "Xtr_cv_full = cv_full.fit_transform(X_train['cleaned_text'])\n",
    "Xte_cv_full = cv_full.transform(X_test['cleaned_text'])\n",
    "\n",
    "# TF-IDF-only on full text\n",
    "tfidf_full = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "Xtr_tfidf_full = tfidf_full.fit_transform(X_train['cleaned_text'])\n",
    "Xte_tfidf_full = tfidf_full.transform(X_test['cleaned_text'])\n",
    "\n",
    "# Final stage voting: P6, CV, TF-IDF\n",
    "final_vote = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('cv_svm', SVC(kernel='linear', probability=True).fit(Xtr_cv_full, y_train)),\n",
    "        ('tfidf_svm', SVC(kernel='linear', probability=True).fit(Xtr_tfidf_full, y_train)),\n",
    "        ('lfs_vote', stage1_vote)\n",
    "    ],\n",
    "    voting='hard'\n",
    ")\n",
    "\n",
    "final_vote.fit(Xtr_cv_full, y_train)  # Fit on one set, predictions from others are internal\n",
    "\n",
    "# Final prediction\n",
    "final_preds = final_vote.predict(Xte_cv_full)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Final Accuracy:\", accuracy_score(y_test, final_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d0e06c",
   "metadata": {},
   "source": [
    "#### Pyspark Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "802b942f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/jars/spark-unsafe_2.12-3.0.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "25/08/19 15:03:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Start Spark session (only once)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Fake News Detection\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"48\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc7f1b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+-----+\n",
      "|index|       cleaned_title|        cleaned_text|label|\n",
      "+-----+--------------------+--------------------+-----+\n",
      "|   31|Credit Suisse Bos...|When Tidjane Thia...|    0|\n",
      "|  265|Angry and inspire...|ROCKVILLE, Md. (R...|    0|\n",
      "|  806|Russian Economy M...|This post was ori...|    1|\n",
      "| 1106|HOUSE SPEAKER PAU...|HOUSE SPEAKER PAU...|    1|\n",
      "| 1151|(AUDIO) RACIST BL...|What is it that t...|    1|\n",
      "+-----+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- index: integer (nullable = true)\n",
      " |-- cleaned_title: string (nullable = true)\n",
      " |-- cleaned_text: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "spark_df = spark.read.csv(\n",
    "    \"cleaned_welfake.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    quote='\"',\n",
    "    escape='\"',\n",
    "    multiLine=True  # needed since your text column contains line breaks\n",
    ")\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "spark_df.show(5)\n",
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58a753fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    0|33668|\n",
      "|    1|25973|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from textblob import TextBlob\n",
    "import textstat\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from pyspark.sql.functions import udf, col, rand, row_number\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, StringType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define schema\n",
    "feature_schema = StructType([\n",
    "    StructField(\"num_special_chars\", IntegerType()),\n",
    "    StructField(\"num_determinants\", IntegerType()),\n",
    "    StructField(\"num_capital_letters\", IntegerType()),\n",
    "    StructField(\"num_short_sentences\", IntegerType()),\n",
    "    StructField(\"num_long_sentences\", IntegerType()),\n",
    "    StructField(\"gunning_fog\", DoubleType()),\n",
    "    StructField(\"smog\", DoubleType()),\n",
    "    StructField(\"ari\", DoubleType()),\n",
    "    StructField(\"polarity\", DoubleType()),\n",
    "    StructField(\"title_similarity\", DoubleType()),\n",
    "    StructField(\"subjectivity\", DoubleType()),\n",
    "    StructField(\"num_syllables\", IntegerType()),\n",
    "    StructField(\"num_words\", IntegerType()),\n",
    "    StructField(\"rate_adj_adv\", DoubleType()),\n",
    "    StructField(\"words_per_sentence\", DoubleType()),\n",
    "    StructField(\"num_articles\", IntegerType()),\n",
    "    StructField(\"num_verbs\", IntegerType()),\n",
    "    StructField(\"num_sentences\", IntegerType()),\n",
    "    StructField(\"num_adjectives\", IntegerType()),\n",
    "    StructField(\"num_adverbs\", IntegerType())\n",
    "])\n",
    "\n",
    "# Feature extraction\n",
    "def extract_features(text):\n",
    "    # try:\n",
    "    tokens = word_tokenize(text)\n",
    "    words = [w for w in tokens if w.isalpha()]\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    pos_tags = pos_tag(words)\n",
    "\n",
    "    num_special_chars = len(re.findall(r'[^a-zA-Z0-9\\s]', text))\n",
    "    num_determinants = sum(1 for w in words if w.lower() in ['the', 'a', 'an'])\n",
    "    num_capital_letters = sum(1 for c in text if c.isupper())\n",
    "    num_short_sent = sum(1 for s in sentences if len(s.split()) < 10)\n",
    "    num_long_sent = sum(1 for s in sentences if len(s.split()) > 20)\n",
    "\n",
    "    gunning_fog = textstat.gunning_fog(text)\n",
    "    smog = textstat.smog_index(text)\n",
    "    ari = textstat.automated_readability_index(text)\n",
    "\n",
    "    blob = TextBlob(text)\n",
    "    polarity = blob.sentiment.polarity\n",
    "    subjectivity = blob.sentiment.subjectivity\n",
    "    title_similarity = 0.0\n",
    "\n",
    "    num_syllables = textstat.syllable_count(text)\n",
    "    num_words = len(words)\n",
    "    num_sentences = len([s for s in sentences if s.strip()])\n",
    "    num_adjectives = sum(1 for _, tag in pos_tags if tag in ['JJ', 'JJR', 'JJS'])\n",
    "    num_adverbs = sum(1 for _, tag in pos_tags if tag in ['RB', 'RBR', 'RBS'])\n",
    "    num_verbs = sum(1 for _, tag in pos_tags if tag.startswith('VB'))\n",
    "    num_articles = sum(1 for w in words if w.lower() in ['a', 'an', 'the'])\n",
    "\n",
    "    rate_adj_adv = (num_adjectives + num_adverbs) / num_words if num_words > 0 else 0\n",
    "    words_per_sent = num_words / num_sentences if num_sentences > 0 else 0\n",
    "\n",
    "    return (\n",
    "        num_special_chars, num_determinants, num_capital_letters, num_short_sent, num_long_sent,\n",
    "        gunning_fog, smog, ari,\n",
    "        polarity, title_similarity, subjectivity,\n",
    "        num_syllables, num_words, rate_adj_adv, words_per_sent,\n",
    "        num_articles, num_verbs, num_sentences, num_adjectives, num_adverbs\n",
    "    )\n",
    "    # except:\n",
    "    #     return (None,) * 20\n",
    "\n",
    "# # --- Stratified sampling with Window (exact N per label) ---\n",
    "# n = 10000  # number of rows per class\n",
    "# w = Window.partitionBy(\"label\").orderBy(rand(seed=42))\n",
    "# df_ranked = spark_df.withColumn(\"row_num\", row_number().over(w))\n",
    "# df_sampled = df_ranked.filter(col(\"row_num\") <= n).drop(\"row_num\")\n",
    "\n",
    "# # Verify stratification\n",
    "# df_sampled.groupBy(\"label\").count().show()\n",
    "spark_df.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fbdcbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: integer (nullable = true)\n",
      " |-- cleaned_title: string (nullable = true)\n",
      " |-- cleaned_text: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- features: struct (nullable = true)\n",
      " |    |-- num_special_chars: integer (nullable = true)\n",
      " |    |-- num_determinants: integer (nullable = true)\n",
      " |    |-- num_capital_letters: integer (nullable = true)\n",
      " |    |-- num_short_sentences: integer (nullable = true)\n",
      " |    |-- num_long_sentences: integer (nullable = true)\n",
      " |    |-- gunning_fog: double (nullable = true)\n",
      " |    |-- smog: double (nullable = true)\n",
      " |    |-- ari: double (nullable = true)\n",
      " |    |-- polarity: double (nullable = true)\n",
      " |    |-- title_similarity: double (nullable = true)\n",
      " |    |-- subjectivity: double (nullable = true)\n",
      " |    |-- num_syllables: integer (nullable = true)\n",
      " |    |-- num_words: integer (nullable = true)\n",
      " |    |-- rate_adj_adv: double (nullable = true)\n",
      " |    |-- words_per_sentence: double (nullable = true)\n",
      " |    |-- num_articles: integer (nullable = true)\n",
      " |    |-- num_verbs: integer (nullable = true)\n",
      " |    |-- num_sentences: integer (nullable = true)\n",
      " |    |-- num_adjectives: integer (nullable = true)\n",
      " |    |-- num_adverbs: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register UDF\n",
    "extract_features_udf = udf(extract_features, feature_schema)\n",
    "\n",
    "# Apply feature extraction\n",
    "# df_with_features = df_sampled.withColumn(\"features\", extract_features_udf(\"cleaned_text\"))\n",
    "df_with_features = spark_df.withColumn(\"features\", extract_features_udf(\"cleaned_text\"))\n",
    "\n",
    "df_with_features.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "949e2543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[index: int, cleaned_title: string, cleaned_text: string, label: int, num_special_chars: int, num_determinants: int, num_capital_letters: int, num_short_sentences: int, num_long_sentences: int, gunning_fog: double, smog: double, ari: double, polarity: double, title_similarity: double, subjectivity: double, num_syllables: int, num_words: int, rate_adj_adv: double, words_per_sentence: double, num_articles: int, num_verbs: int, num_sentences: int, num_adjectives: int, num_adverbs: int]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Flatten struct in one go instead of looping with withColumn\n",
    "df_with_features = df_with_features.select(\n",
    "    \"*\",        # keep all existing columns\n",
    "    F.col(\"features.*\")  # expand all fields inside \"features\"\n",
    ").drop(\"features\").na.drop(how=\"any\")  # drop original struct column\n",
    "\n",
    "# # Drop rows with null values\n",
    "# df_with_features = df_with_features.na.drop(how=\"any\")\n",
    "\n",
    "df_with_features.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856f75b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Select, sanitize, and write a single safely-quoted CSV\n",
    "temp_path = \"temp_csv_output\"\n",
    "final_path = \"cleaned_welfake_with_features.csv\"\n",
    "\n",
    "# Write as a single part with strict quoting\n",
    "(\n",
    "    df_with_features\n",
    "    .coalesce(1)\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"header\", True)\n",
    "    .option(\"quote\", '\"')        # use \" as quote char\n",
    "    .option(\"escape\", '\"')       # escape \" as \"\"\n",
    "    .option(\"quoteAll\", True)    # quote every field to be extra safe\n",
    "    .csv(temp_path)\n",
    ")\n",
    "\n",
    "# Find the part file inside the temp folder and move it as the final CSV\n",
    "for file in os.listdir(temp_path):\n",
    "    if file.startswith(\"part-\") and file.endswith(\".csv\"):\n",
    "        # remove existing file if present (optional but safer across OSes)\n",
    "        if os.path.exists(final_path):\n",
    "            os.remove(final_path)\n",
    "        shutil.move(os.path.join(temp_path, file), final_path)\n",
    "        break\n",
    "\n",
    "# Clean up\n",
    "shutil.rmtree(temp_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99fedbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):==========================>     (18 + 2) / 20]\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n",
      "    process()\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 223, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 212, in _batched\n",
      "    for item in iterator:\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 88, in <lambda>\n",
      "    return lambda *a: toInternal(f(*a))\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/var/folders/43/3cgswvds18x86rjptw_qfpxw0000gn/T/ipykernel_14530/2512194588.py\", line 41, in extract_features\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/nltk/tag/__init__.py\", line 169, in pos_tag\n",
      "    return _pos_tag(tokens, tagset, tagger, lang)\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/nltk/tag/__init__.py\", line 126, in _pos_tag\n",
      "    tagged_tokens = tagger.tag(tokens)\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/nltk/tag/perceptron.py\", line 201, in tag\n",
      "    tag, conf = self.model.predict(features, return_conf)\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/nltk/tag/perceptron.py\", line 83, in predict\n",
      "    scores[label] += value * weight\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n",
      "    process()\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 223, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 212, in _batched\n",
      "    for item in iterator:\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 88, in <lambda>\n",
      "    return lambda *a: toInternal(f(*a))\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/var/folders/43/3cgswvds18x86rjptw_qfpxw0000gn/T/ipykernel_14530/2512194588.py\", line 37, in extract_features\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/nltk/tokenize/__init__.py\", line 143, in word_tokenize\n",
      "    return [\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/nltk/tokenize/__init__.py\", line 144, in <listcomp>\n",
      "    token for sent in sentences for token in _treebank_word_tokenizer.tokenize(sent)\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/nltk/tokenize/destructive.py\", line 182, in tokenize\n",
      "    text = regexp.sub(r\" \\1 \\2 \", text)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n",
      "    process()\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 223, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 212, in _batched\n",
      "    for item in iterator:\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 88, in <lambda>\n",
      "    return lambda *a: toInternal(f(*a))\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/var/folders/43/3cgswvds18x86rjptw_qfpxw0000gn/T/ipykernel_14530/2512194588.py\", line 53, in extract_features\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/textblob/decorators.py\", line 23, in __get__\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/textblob/blob.py\", line 439, in sentiment\n",
      "    return self.analyzer.analyze(self.raw)\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/textblob/en/sentiments.py\", line 45, in analyze\n",
      "    return Sentiment(*pattern_sentiment(text))\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/textblob/_text.py\", line 1000, in __call__\n",
      "    a = self.assessments(\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/textblob/_text.py\", line 1123, in assessments\n",
      "    if w in map(lambda e: e.lower(), e):\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n",
      "    process()\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 223, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 212, in _batched\n",
      "    for item in iterator:\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 88, in <lambda>\n",
      "    return lambda *a: toInternal(f(*a))\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/var/folders/43/3cgswvds18x86rjptw_qfpxw0000gn/T/ipykernel_14530/2512194588.py\", line 41, in extract_features\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/nltk/tag/__init__.py\", line 169, in pos_tag\n",
      "    return _pos_tag(tokens, tagset, tagger, lang)\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/nltk/tag/__init__.py\", line 126, in _pos_tag\n",
      "    tagged_tokens = tagger.tag(tokens)\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/nltk/tag/perceptron.py\", line 201, in tag\n",
      "    tag, conf = self.model.predict(features, return_conf)\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/nltk/tag/perceptron.py\", line 83, in predict\n",
      "    scores[label] += value * weight\n",
      "KeyboardInterrupt\n",
      "25/08/19 15:03:46 ERROR PythonUDFRunner: Python worker exited unexpectedly (crashed)\n",
      "java.io.EOFException\n",
      "\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:397)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:71)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$1.next(InMemoryRelation.scala:98)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$1.next(InMemoryRelation.scala:90)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:222)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1371)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1298)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1362)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1186)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:360)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:311)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "25/08/19 15:03:46 ERROR PythonUDFRunner: Python worker exited unexpectedly (crashed)\n",
      "java.net.SocketException: Connection reset\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:186)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)\n",
      "\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)\n",
      "\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:71)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:511)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1158)\n",
      "\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1174)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1212)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n",
      "25/08/19 15:03:46 ERROR Executor: Exception in task 14.0 in stage 16.0 (TID 59)\n",
      "org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:536)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:525)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:86)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$1.next(InMemoryRelation.scala:98)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$1.next(InMemoryRelation.scala:90)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:222)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1371)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1298)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1362)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1186)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:360)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:311)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:397)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:71)\n",
      "\t... 37 more\n",
      "25/08/19 15:03:46 ERROR PythonUDFRunner: This may have been caused by a prior exception:\n",
      "java.net.SocketException: Connection reset\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:186)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)\n",
      "\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)\n",
      "\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:71)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:511)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n",
      "25/08/19 15:03:46 ERROR PythonUDFRunner: This may have been caused by a prior exception:\n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)\n",
      "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:283)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:295)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:295)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n",
      "25/08/19 15:03:46 ERROR Executor: Exception in task 10.0 in stage 16.0 (TID 58)\n",
      "java.net.SocketException: Connection reset\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:186)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)\n",
      "\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)\n",
      "\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:71)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:511)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n",
      "25/08/19 15:03:46 ERROR TaskSetManager: Task 10 in stage 16.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Apply UDF to smaller sample\u001b[39;00m\n\u001b[1;32m      2\u001b[0m sampled \u001b[38;5;241m=\u001b[39m df_with_features\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;241m0.001\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43msampled\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/sql/dataframe.py:440\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    407\u001b[0m \n\u001b[1;32m    408\u001b[0m \u001b[38;5;124;03m:param n: Number of rows to show.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;124;03m name | Bob\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 440\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mshowString(n, \u001b[38;5;28mint\u001b[39m(truncate), vertical))\n",
      "File \u001b[0;32m~/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/py4j/java_gateway.py:1303\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1296\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1305\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/py4j/java_gateway.py:1033\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1033\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1035\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/py4j/java_gateway.py:1200\u001b[0m, in \u001b[0;36mGatewayConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JNetworkError(\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while sending\u001b[39m\u001b[38;5;124m\"\u001b[39m, e, proto\u001b[38;5;241m.\u001b[39mERROR_ON_SEND)\n\u001b[1;32m   1199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1200\u001b[0m     answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m   1201\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m   1202\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m answer\u001b[38;5;241m.\u001b[39mstartswith(proto\u001b[38;5;241m.\u001b[39mRETURN_MESSAGE):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Apply UDF to smaller sample\n",
    "sampled = df_with_features.sample(0.001, seed=42)\n",
    "sampled.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e7caf21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:===============================>                        (13 + 8) / 23]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+------------+-----+-----------------+----------------+-------------------+-------------------+------------------+-----------+----+---+--------+----------------+------------+-------------+---------+------------+------------------+------------+---------+-------------+--------------+-----------+\n",
      "|index|cleaned_title|cleaned_text|label|num_special_chars|num_determinants|num_capital_letters|num_short_sentences|num_long_sentences|gunning_fog|smog|ari|polarity|title_similarity|subjectivity|num_syllables|num_words|rate_adj_adv|words_per_sentence|num_articles|num_verbs|num_sentences|num_adjectives|num_adverbs|\n",
      "+-----+-------------+------------+-----+-----------------+----------------+-------------------+-------------------+------------------+-----------+----+---+--------+----------------+------------+-------------+---------+------------+------------------+------------+---------+-------------+--------------+-----------+\n",
      "+-----+-------------+------------+-----+-----------------+----------------+-------------------+-------------------+------------------+-----------+----+---+--------+----------------+------------+-------------+---------+------------+------------------+------------+---------+-------------+--------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_with_features.sample(0.0000001).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95024da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark UI: http://192.168.0.47:4040\n"
     ]
    }
   ],
   "source": [
    "# Get the Spark UI URL\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")\n",
    "# Usually: http://localhost:4040 or http://driver-ip:4040"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af438dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:==================================================>     (18 + 2) / 20]\r"
     ]
    }
   ],
   "source": [
    "df_with_features.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0da410",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_features.sample(0.01).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f048c5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+-----+-----------------+----------------+-------------------+-------------------+------------------+------------------+------------------+------------------+--------------------+----------------+-------------------+-------------+---------+-------------------+------------------+------------+---------+-------------+--------------+-----------+\n",
      "|index|       cleaned_title|        cleaned_text|label|num_special_chars|num_determinants|num_capital_letters|num_short_sentences|num_long_sentences|       gunning_fog|              smog|               ari|            polarity|title_similarity|       subjectivity|num_syllables|num_words|       rate_adj_adv|words_per_sentence|num_articles|num_verbs|num_sentences|num_adjectives|num_adverbs|\n",
      "+-----+--------------------+--------------------+-----+-----------------+----------------+-------------------+-------------------+------------------+------------------+------------------+------------------+--------------------+----------------+-------------------+-------------+---------+-------------------+------------------+------------+---------+-------------+--------------+-----------+\n",
      "|41561|Factbox: Trump ta...|(Reuters) - The r...|    0|              154|              56|                119|                 14|                20| 14.55739354036565|  13.6195642746484|13.528094714587738| 0.07933875983171759|             0.0| 0.4267294677153831|         1355|      778|0.11696658097686376|14.961538461538462|          56|      148|           52|            68|         23|\n",
      "|37717|Trump handling of...|WASHINGTON (Reute...|    0|               55|              32|                 96|                  4|                11| 19.68131303144157|17.032423316055382|18.748153846153848|0.021230158730158728|             0.0|0.24682539682539684|          661|      383|0.08093994778067885|27.357142857142858|          32|       67|           14|            25|          6|\n",
      "|41701|Russia ready to c...|MOSCOW (Reuters) ...|    0|               20|               5|                 34|                  4|                 1|11.383291139240507|11.602472056035307|          11.55075|0.002083333333333335|             0.0|0.20833333333333334|          131|       77|0.07792207792207792|              11.0|           5|       11|            7|             6|          0|\n",
      "|20427|Michael Flynn, Ki...|(Want to get this...|    0|              230|              87|                154|                 51|                15| 11.24025974025974|11.315629713498875|  9.22396718146718| 0.11907819111208941|             0.0| 0.4917327023259227|         1348|      824|0.10558252427184465| 9.694117647058823|          87|      146|           85|            58|         29|\n",
      "|27515|On Sundays in Sen...|The rest of the w...|    0|               42|              48|                 31|                  2|                 9|12.226138909634056|11.661520659325951| 11.99906699751861| 0.14558080808080812|             0.0|0.41397630147630143|          445|      308|0.12337662337662338|23.692307692307693|          48|       41|           13|            23|         15|\n",
      "+-----+--------------------+--------------------+-----+-----------------+----------------+-------------------+-------------------+------------------+------------------+------------------+------------------+--------------------+----------------+-------------------+-------------+---------+-------------------+------------------+------------+---------+-------------+--------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2d7b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):==========================>     (18 + 2) / 20]\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n",
      "    process()\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 223, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 212, in _batched\n",
      "    for item in iterator:\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 88, in <lambda>\n",
      "    return lambda *a: toInternal(f(*a))\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/var/folders/43/3cgswvds18x86rjptw_qfpxw0000gn/T/ipykernel_13888/2512194588.py\", line 37, in extract_features\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/nltk/tokenize/__init__.py\", line 143, in word_tokenize\n",
      "    return [\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/nltk/tokenize/__init__.py\", line 144, in <listcomp>\n",
      "    token for sent in sentences for token in _treebank_word_tokenizer.tokenize(sent)\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/nltk/tokenize/destructive.py\", line 158, in tokenize\n",
      "    text = regexp.sub(substitution, text)\n",
      "  File \"/Users/bachtiarherdianto/.pyenv/versions/3.10.14/lib/python3.10/re.py\", line 330, in filter\n",
      "    def filter(match, template=template):\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n",
      "    process()\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 223, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 212, in _batched\n",
      "    for item in iterator:\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 88, in <lambda>\n",
      "    return lambda *a: toInternal(f(*a))\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/var/folders/43/3cgswvds18x86rjptw_qfpxw0000gn/T/ipykernel_13888/2512194588.py\", line 48, in extract_features\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/textstat/textstat.py\", line 878, in gunning_fog\n",
      "    return self._legacy_round(metrics.gunning_fog(text, self.__lang))\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/textstat/backend/metrics/_gunning_fog.py\", line 28, in gunning_fog\n",
      "    diff_words = count_difficult_words(text, lang, syllable_threshold)\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/textstat/backend/counts/_count_difficult_words.py\", line 36, in count_difficult_words\n",
      "    return len(list_difficult_words(text, syllable_threshold, lang))\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/textstat/backend/selections/_list_difficult_words.py\", line 30, in list_difficult_words\n",
      "    diff_words = [\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/textstat/backend/selections/_list_difficult_words.py\", line 31, in <listcomp>\n",
      "    word for word in words if is_difficult_word(word, syllable_threshold, lang)\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/textstat/backend/validations/_is_difficult_word.py\", line 48, in is_difficult_word\n",
      "    if count_syllables(word, lang) < syllable_threshold:\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/textstat/backend/counts/_count_syllables.py\", line 28, in count_syllables\n",
      "    for word in list_words(text, lowercase=True):\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/textstat/backend/selections/_list_words.py\", line 51, in list_words\n",
      "    text = remove_punctuation(text, rm_apostrophe=rm_apostrophe)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 467, in main\n",
      "    split_index = read_int(infile)\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 593, in read_int\n",
      "    length = stream.read(4)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 467, in main\n",
      "    split_index = read_int(infile)\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 593, in read_int\n",
      "    length = stream.read(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 467, in main\n",
      "    split_index = read_int(infile)\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 593, in read_int\n",
      "    length = stream.read(4)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 467, in main\n",
      "    split_index = read_int(infile)\n",
      "  File \"/Users/bachtiarherdianto/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 593, in read_int\n",
      "    length = stream.read(4)\n",
      "KeyboardInterrupt\n",
      "25/08/19 14:43:51 ERROR PythonUDFRunner: Python worker exited unexpectedly (crashed)\n",
      "java.net.SocketException: Connection reset\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:186)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)\n",
      "\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)\n",
      "\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:71)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$1.next(InMemoryRelation.scala:98)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$1.next(InMemoryRelation.scala:90)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:222)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1371)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1298)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1362)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1186)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:360)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:311)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:362)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1371)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1298)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1362)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1186)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:360)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:311)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "25/08/19 14:43:51 ERROR PythonUDFRunner: This may have been caused by a prior exception:\n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)\n",
      "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:283)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:295)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:295)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n",
      "25/08/19 14:43:51 ERROR Executor: Exception in task 14.0 in stage 16.0 (TID 59)\n",
      "java.net.SocketException: Connection reset\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:186)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)\n",
      "\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)\n",
      "\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:71)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$1.next(InMemoryRelation.scala:98)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$1.next(InMemoryRelation.scala:90)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:222)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1371)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1298)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1362)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1186)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:360)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:311)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:362)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1371)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1298)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1362)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1186)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:360)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:311)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "25/08/19 14:43:51 ERROR Executor: Exception in task 10.0 in stage 16.0 (TID 58)\n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)\n",
      "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:283)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:295)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:295)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n",
      "25/08/19 14:43:52 ERROR TaskSetManager: Task 10 in stage 16.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Convert to Pandas DataFrame for easier viewing\u001b[39;00m\n\u001b[1;32m      4\u001b[0m pd\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisplay.max_columns\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# Show all columns\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mdf_with_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m, columns\u001b[38;5;241m=\u001b[39mdf_with_features\u001b[38;5;241m.\u001b[39mcolumns)\n",
      "File \u001b[0;32m~/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/sql/dataframe.py:639\u001b[0m, in \u001b[0;36mDataFrame.take\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;129m@ignore_unicode_prefix\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;129m@since\u001b[39m(\u001b[38;5;241m1.3\u001b[39m)\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtake\u001b[39m(\u001b[38;5;28mself\u001b[39m, num):\n\u001b[1;32m    634\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the first ``num`` rows as a :class:`list` of :class:`Row`.\u001b[39;00m\n\u001b[1;32m    635\u001b[0m \n\u001b[1;32m    636\u001b[0m \u001b[38;5;124;03m    >>> df.take(2)\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;124;03m    [Row(age=2, name=u'Alice'), Row(age=5, name=u'Bob')]\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/pyspark/sql/dataframe.py:596\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m    591\u001b[0m \n\u001b[1;32m    592\u001b[0m \u001b[38;5;124;03m>>> df.collect()\u001b[39;00m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;124;03m[Row(age=2, name=u'Alice'), Row(age=5, name=u'Bob')]\u001b[39;00m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc) \u001b[38;5;28;01mas\u001b[39;00m css:\n\u001b[0;32m--> 596\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(PickleSerializer())))\n",
      "File \u001b[0;32m~/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/py4j/java_gateway.py:1303\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1296\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1305\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/py4j/java_gateway.py:1033\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1033\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1035\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/Documents/Projects/fake-news-detection/.venv/lib/python3.10/site-packages/py4j/java_gateway.py:1200\u001b[0m, in \u001b[0;36mGatewayConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JNetworkError(\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while sending\u001b[39m\u001b[38;5;124m\"\u001b[39m, e, proto\u001b[38;5;241m.\u001b[39mERROR_ON_SEND)\n\u001b[1;32m   1199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1200\u001b[0m     answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m   1201\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m   1202\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m answer\u001b[38;5;241m.\u001b[39mstartswith(proto\u001b[38;5;241m.\u001b[39mRETURN_MESSAGE):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert to Pandas DataFrame for easier viewing\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.DataFrame(df_with_features.take(5), columns=df_with_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af49fe9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>cleaned_title</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>label</th>\n",
       "      <th>num_special_chars</th>\n",
       "      <th>num_determinants</th>\n",
       "      <th>num_capital_letters</th>\n",
       "      <th>num_short_sentences</th>\n",
       "      <th>num_long_sentences</th>\n",
       "      <th>gunning_fog</th>\n",
       "      <th>smog</th>\n",
       "      <th>ari</th>\n",
       "      <th>polarity</th>\n",
       "      <th>title_similarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>num_syllables</th>\n",
       "      <th>num_words</th>\n",
       "      <th>rate_adj_adv</th>\n",
       "      <th>words_per_sentence</th>\n",
       "      <th>num_articles</th>\n",
       "      <th>num_verbs</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>num_adjectives</th>\n",
       "      <th>num_adverbs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26903</td>\n",
       "      <td>U.S. gun rules heighten tension between police...</td>\n",
       "      <td>WARSAW (Reuters) - President Barack Obama pled...</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>40</td>\n",
       "      <td>73</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>14.814928</td>\n",
       "      <td>13.409020</td>\n",
       "      <td>14.159571</td>\n",
       "      <td>0.004114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.380217</td>\n",
       "      <td>836</td>\n",
       "      <td>538</td>\n",
       "      <td>0.089219</td>\n",
       "      <td>23.391304</td>\n",
       "      <td>40</td>\n",
       "      <td>97</td>\n",
       "      <td>23</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48590</td>\n",
       "      <td>Yemen says Saudi-led coalition to allow commer...</td>\n",
       "      <td>ADEN (Reuters) - The Saudi-led military coalit...</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>25</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>17.268550</td>\n",
       "      <td>15.760457</td>\n",
       "      <td>16.741491</td>\n",
       "      <td>-0.066667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.131250</td>\n",
       "      <td>411</td>\n",
       "      <td>226</td>\n",
       "      <td>0.097345</td>\n",
       "      <td>22.600000</td>\n",
       "      <td>25</td>\n",
       "      <td>34</td>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31781</td>\n",
       "      <td>Yemen air strike kills eight women, two childr...</td>\n",
       "      <td>DUBAI (Reuters) - Eight women and two children...</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>23</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>17.298246</td>\n",
       "      <td>15.470042</td>\n",
       "      <td>17.657417</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.221510</td>\n",
       "      <td>300</td>\n",
       "      <td>183</td>\n",
       "      <td>0.049180</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>23</td>\n",
       "      <td>39</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8921</td>\n",
       "      <td>Donald Trump says he doesn't need a unified GO...</td>\n",
       "      <td>When Donald Trump told ABC's George Stephanopo...</td>\n",
       "      <td>0</td>\n",
       "      <td>180</td>\n",
       "      <td>64</td>\n",
       "      <td>154</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>12.797101</td>\n",
       "      <td>12.709667</td>\n",
       "      <td>10.767404</td>\n",
       "      <td>0.151101</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.472443</td>\n",
       "      <td>1412</td>\n",
       "      <td>903</td>\n",
       "      <td>0.125138</td>\n",
       "      <td>18.428571</td>\n",
       "      <td>64</td>\n",
       "      <td>166</td>\n",
       "      <td>49</td>\n",
       "      <td>66</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14733</td>\n",
       "      <td>All the Clamor? Trump's Palm Beach Neighbors S...</td>\n",
       "      <td>PALM BEACH, Fla. There will be no this weekend...</td>\n",
       "      <td>0</td>\n",
       "      <td>188</td>\n",
       "      <td>79</td>\n",
       "      <td>169</td>\n",
       "      <td>32</td>\n",
       "      <td>20</td>\n",
       "      <td>11.510657</td>\n",
       "      <td>11.789604</td>\n",
       "      <td>10.009782</td>\n",
       "      <td>0.047362</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.414748</td>\n",
       "      <td>1416</td>\n",
       "      <td>890</td>\n",
       "      <td>0.103371</td>\n",
       "      <td>12.535211</td>\n",
       "      <td>79</td>\n",
       "      <td>161</td>\n",
       "      <td>71</td>\n",
       "      <td>65</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                      cleaned_title  \\\n",
       "0  26903  U.S. gun rules heighten tension between police...   \n",
       "1  48590  Yemen says Saudi-led coalition to allow commer...   \n",
       "2  31781  Yemen air strike kills eight women, two childr...   \n",
       "3   8921  Donald Trump says he doesn't need a unified GO...   \n",
       "4  14733  All the Clamor? Trump's Palm Beach Neighbors S...   \n",
       "\n",
       "                                        cleaned_text  label  \\\n",
       "0  WARSAW (Reuters) - President Barack Obama pled...      0   \n",
       "1  ADEN (Reuters) - The Saudi-led military coalit...      0   \n",
       "2  DUBAI (Reuters) - Eight women and two children...      0   \n",
       "3  When Donald Trump told ABC's George Stephanopo...      0   \n",
       "4  PALM BEACH, Fla. There will be no this weekend...      0   \n",
       "\n",
       "   num_special_chars  num_determinants  num_capital_letters  \\\n",
       "0                 67                40                   73   \n",
       "1                 31                25                   61   \n",
       "2                 26                23                   28   \n",
       "3                180                64                  154   \n",
       "4                188                79                  169   \n",
       "\n",
       "   num_short_sentences  num_long_sentences  gunning_fog       smog        ari  \\\n",
       "0                    4                  15    14.814928  13.409020  14.159571   \n",
       "1                    2                   5    17.268550  15.760457  16.741491   \n",
       "2                    1                   6    17.298246  15.470042  17.657417   \n",
       "3                   12                  19    12.797101  12.709667  10.767404   \n",
       "4                   32                  20    11.510657  11.789604  10.009782   \n",
       "\n",
       "   polarity  title_similarity  subjectivity  num_syllables  num_words  \\\n",
       "0  0.004114               0.0      0.380217            836        538   \n",
       "1 -0.066667               0.0      0.131250            411        226   \n",
       "2  0.066667               0.0      0.221510            300        183   \n",
       "3  0.151101               0.0      0.472443           1412        903   \n",
       "4  0.047362               0.0      0.414748           1416        890   \n",
       "\n",
       "   rate_adj_adv  words_per_sentence  num_articles  num_verbs  num_sentences  \\\n",
       "0      0.089219           23.391304            40         97             23   \n",
       "1      0.097345           22.600000            25         34             10   \n",
       "2      0.049180           30.500000            23         39              6   \n",
       "3      0.125138           18.428571            64        166             49   \n",
       "4      0.103371           12.535211            79        161             71   \n",
       "\n",
       "   num_adjectives  num_adverbs  \n",
       "0              32           16  \n",
       "1              19            3  \n",
       "2               6            3  \n",
       "3              66           47  \n",
       "4              65           27  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Flatten struct\n",
    "for col_name in feature_schema.fieldNames():\n",
    "    df_with_features = df_with_features.withColumn(col_name, df_with_features[\"features\"][col_name])\n",
    "\n",
    "df_with_features = df_with_features.drop(\"features\").na.drop(how=\"any\")\n",
    "\n",
    "# Convert to Pandas DataFrame for easier viewing\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.DataFrame(df_with_features.take(5), columns=df_with_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b323dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:=====================================================>  (46 + 2) / 48]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with at least one null/NaN feature: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Define features\n",
    "feature_cols = [\n",
    "    'num_special_chars', 'num_determinants', 'num_capital_letters', 'num_short_sentences', 'num_long_sentences',\n",
    "    'gunning_fog', 'smog', 'ari',\n",
    "    'polarity', 'title_similarity', 'subjectivity',\n",
    "    'num_syllables', 'num_words', 'rate_adj_adv', 'words_per_sentence',\n",
    "    'num_articles', 'num_verbs', 'num_sentences', 'num_adjectives', 'num_adverbs'\n",
    "]\n",
    "\n",
    "# Build condition: check if any feature column is null or NaN\n",
    "condition = None\n",
    "for c in feature_cols:\n",
    "    col_cond = F.col(c).isNull() | F.isnan(c)\n",
    "    condition = col_cond if condition is None else (condition | col_cond)\n",
    "\n",
    "# Count rows with at least one null/NaN\n",
    "null_count = df_with_features.filter(condition).count()\n",
    "print(f\"Rows with at least one null/NaN feature: {null_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4203893e",
   "metadata": {},
   "source": [
    "##### Classic ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a181e792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[index: int, cleaned_title: string, cleaned_text: string, label: int, num_special_chars: int, num_determinants: int, num_capital_letters: int, num_short_sentences: int, num_long_sentences: int, gunning_fog: double, smog: double, ari: double, polarity: double, title_similarity: double, subjectivity: double, num_syllables: int, num_words: int, rate_adj_adv: double, words_per_sentence: double, num_articles: int, num_verbs: int, num_sentences: int, num_adjectives: int, num_adverbs: int]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import (\n",
    "    LogisticRegression, DecisionTreeClassifier, RandomForestClassifier,\n",
    "    GBTClassifier, NaiveBayes\n",
    ")\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "# Define features\n",
    "feature_cols = [\n",
    "    'num_special_chars', 'num_determinants', 'num_capital_letters', 'num_short_sentences', 'num_long_sentences',\n",
    "    'gunning_fog', 'smog', 'ari',\n",
    "    'polarity', 'title_similarity', 'subjectivity',\n",
    "    'num_syllables', 'num_words', 'rate_adj_adv', 'words_per_sentence',\n",
    "    'num_articles', 'num_verbs', 'num_sentences', 'num_adjectives', 'num_adverbs'\n",
    "]\n",
    "\n",
    "# Assemble features into vector\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_vec\", handleInvalid=\"skip\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler(inputCol=\"features_vec\", outputCol=\"features\", withStd=True, withMean=True)\n",
    "\n",
    "# Train/test split\n",
    "train_df, test_df = df_with_features.randomSplit([0.8, 0.2], seed=42)\n",
    "train_df.cache()\n",
    "test_df.cache()\n",
    "\n",
    "# Models available in Spark MLlib\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=100),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"label\"),\n",
    "    \"Random Forest\": RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=100),\n",
    "    \"Gradient Boosting\": GBTClassifier(featuresCol=\"features\", labelCol=\"label\", maxIter=100),\n",
    "    \"Naive Bayes\": NaiveBayes(featuresCol=\"features\", labelCol=\"label\", modelType=\"gaussian\")\n",
    "}\n",
    "\n",
    "# Evaluators\n",
    "bin_eval = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "multi_eval = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, clf in models.items():\n",
    "    pipeline = Pipeline(stages=[assembler, scaler, clf])\n",
    "    model = pipeline.fit(train_df)\n",
    "    preds = model.transform(test_df)\n",
    "\n",
    "    acc = multi_eval.setMetricName(\"accuracy\").evaluate(preds)\n",
    "    f1 = multi_eval.setMetricName(\"f1\").evaluate(preds)\n",
    "    precision = multi_eval.setMetricName(\"weightedPrecision\").evaluate(preds)\n",
    "    recall = multi_eval.setMetricName(\"weightedRecall\").evaluate(preds)\n",
    "    auc = bin_eval.evaluate(preds)\n",
    "\n",
    "    results.append((name, acc, precision, recall, f1, auc))\n",
    "\n",
    "# Convert results to Spark DataFrame\n",
    "results_df = spark.createDataFrame(results, [\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"AUC\"]).toPandas()\n",
    "results_df.sort_values('Accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d755e8",
   "metadata": {},
   "source": [
    "##### WELFake Approach"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "welfake-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
